{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbaae29",
   "metadata": {},
   "source": [
    "# Regression I: K-nearest neighbors {#regression1}\n",
    "\n",
    "```{r regression1-setup, echo = FALSE, message = FALSE, warning = FALSE}\n",
    "library(knitr)\n",
    "library(plotly)\n",
    "library(stringr)\n",
    "\n",
    "knitr::opts_chunk$set(fig.align = \"center\")\n",
    "reticulate::use_miniconda('r-reticulate')\n",
    "\n",
    "print_tidymodels <- function(tidymodels_object) {\n",
    "  if(!is_latex_output()) {\n",
    "    tidymodels_object\n",
    "  } else {\n",
    "    output <- capture.output(tidymodels_object)\n",
    "    \n",
    "    for (i in seq_along(output)) {\n",
    "      if (nchar(output[i]) <= 80) {\n",
    "        cat(output[i], sep = \"\\n\")\n",
    "      } else {\n",
    "        cat(str_sub(output[i], start = 1, end = 80), sep = \"\\n\")\n",
    "        cat(str_sub(output[i], start = 81, end = nchar(output[i])), sep = \"\\n\")\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "theme_update(axis.title = element_text(size = 12)) # modify axis label size in plots \n",
    "```\n",
    "\n",
    "## Overview \n",
    "\n",
    "This chapter continues our foray into answering predictive questions.\n",
    "Here we will focus on predicting *numerical* variables \n",
    "and will use *regression* to perform this task.\n",
    "This is unlike the past two chapters, which focused on predicting categorical\n",
    "variables via classification. However, regression does have many similarities\n",
    "to classification: for example, just as in the case of classification,\n",
    "we will split our data into training, validation, and test sets, we will \n",
    "use `tidymodels` workflows, we will use a K-nearest neighbors (KNN) \n",
    "approach to make predictions, and we will use cross-validation to choose K.\n",
    "Because of how similar these procedures are, make sure to read Chapters\n",
    "\\@ref(classification) and \\@ref(classification2) before reading \n",
    "this one&mdash;we will move a little bit faster here with the\n",
    "concepts that have already been covered.\n",
    "This chapter will primarily focus on the case where there is a single predictor, \n",
    "but the end of the chapter shows how to perform\n",
    "regression with more than one predictor variable, i.e., *multivariable regression*.\n",
    "It is important to note that regression \n",
    "can also be used to answer inferential and causal questions, \n",
    "however that is beyond the scope of this book.\n",
    "\n",
    "## Chapter learning objectives \n",
    "By the end of the chapter, readers will be able to do the following:\n",
    "\n",
    "* Recognize situations where a simple regression analysis would be appropriate for making predictions.\n",
    "* Explain the K-nearest neighbor (KNN) regression algorithm and describe how it differs from KNN classification.\n",
    "* Interpret the output of a KNN regression.\n",
    "* In a dataset with two or more variables, perform K-nearest neighbor regression in R using a `tidymodels` workflow.\n",
    "* Execute cross-validation in R to choose the number of neighbors.\n",
    "* Evaluate KNN regression prediction accuracy in R using a test data set and the root mean squared prediction error (RMSPE).\n",
    "* In the context of KNN regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).\n",
    "* Describe the advantages and disadvantages of K-nearest neighbors regression.\n",
    "\n",
    "## The regression problem\n",
    "\n",
    "Regression, like classification, is a predictive \\index{predictive question} problem setting where we want\n",
    "to use past information to predict future observations. But in the case of\n",
    "regression, the goal is to predict *numerical* values instead of *categorical* values. \n",
    "The variable that you want to predict is often called the *response variable*. \\index{response variable}\n",
    "For example, we could try to use the number of hours a person spends on\n",
    "exercise each week to predict their race time in the annual Boston marathon. As \n",
    "another example, we could try to use the size of a house to\n",
    "predict its sale price. Both of these response variables&mdash;race time and sale price&mdash;are \n",
    "numerical, and so predicting them given past data is considered a regression problem.\n",
    "\n",
    "Just like in the \\index{classification!comparison to regression} \n",
    "classification setting, there are many possible methods that we can use \n",
    "to predict numerical response variables. In this chapter we will\n",
    "focus on the **K-nearest neighbors** algorithm [@knnfix; @knncover], and in the next chapter\n",
    "we will study **linear regression**.\n",
    "In your future studies, you might encounter regression trees, splines,\n",
    "and general local regression methods; see the additional resources\n",
    "section at the end of the next chapter for where to begin learning more about\n",
    "these other methods. \n",
    "\n",
    "Many of the concepts from classification map over to the setting of regression. For example, \n",
    "a regression model predicts a new observation's response variable based on the response variables\n",
    "for similar observations in the data set of past observations. When building a regression model,\n",
    "we first split the data into training and test sets, in order to ensure that we assess the performance\n",
    "of our method on observations not seen during training. And finally, we can use cross-validation to evaluate different\n",
    "choices of model parameters (e.g., K in a K-nearest neighbors model). The major difference\n",
    "is that we are now predicting numerical variables instead of categorical variables.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "> **Note:** You can usually tell whether a\\index{categorical variable}\\index{numerical variable} variable is numerical or\n",
    "> categorical&mdash;and therefore whether you need to perform regression or\n",
    "> classification&mdash;by taking two response variables X and Y from your data,\n",
    "> and asking the question, \"is response variable X *more* than response\n",
    "> variable Y?\" If the variable is categorical, the question will make no sense.\n",
    "> (Is blue more than red?  Is benign more than malignant?) If the variable is\n",
    "> numerical, it will make sense. (Is 1.5 hours more than 2.25 hours? Is\n",
    "> \\$500,000 more than \\$400,000?) Be careful when applying this heuristic,\n",
    "> though: sometimes categorical variables will be encoded as numbers in your\n",
    "> data (e.g., \"1\" represents \"benign\", and \"0\" represents \"malignant\"). In\n",
    "> these cases you have to ask the question about the *meaning* of the labels\n",
    "> (\"benign\" and \"malignant\"), not their values (\"1\" and \"0\"). \n",
    "\n",
    "## Exploring a data set\n",
    "\n",
    "In this chapter and the next, we will study \n",
    "a data set \\index{Sacramento real estate} of \n",
    "[932 real estate transactions in Sacramento, California](https://support.spatialkey.com/spatialkey-sample-csv-data/) \n",
    "originally reported in the *Sacramento Bee* newspaper.\n",
    "We first need to formulate a precise question that\n",
    "we want to answer. In this example, our question is again predictive:\n",
    "\\index{question!regression} Can we use the size of a house in the Sacramento, CA area to predict\n",
    "its sale price? A rigorous, quantitative answer to this question might help\n",
    "a realtor advise a client as to whether the price of a particular listing \n",
    "is fair, or perhaps how to set the price of a new listing.\n",
    "We begin the analysis by loading and examining the data, and setting the seed value.\n",
    "\n",
    "\\index{seed!set.seed}\n",
    "\n",
    "```{r 07-load, message = FALSE}\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(gridExtra)\n",
    "\n",
    "set.seed(5)\n",
    "\n",
    "sacramento <- read_csv(\"data/sacramento.csv\")\n",
    "sacramento\n",
    "```\n",
    "\n",
    "The scientific question guides our initial exploration: the columns in the\n",
    "data that we are interested in are `sqft` (house size, in livable square feet)\n",
    "and `price` (house sale price, in US dollars (USD)).  The first step is to visualize\n",
    "the data as a scatter plot where we place the predictor variable\n",
    "(house size) on the x-axis, and we place the target/response variable that we\n",
    "want to predict (sale price) on the y-axis.\n",
    "\\index{ggplot!geom\\_point}\n",
    "\\index{visualization!scatter}\n",
    "\n",
    "> **Note:** Given that the y-axis unit is dollars in Figure \\@ref(fig:07-edaRegr), \n",
    "> we format the axis labels to put dollar signs in front of the house prices, \n",
    "> as well as commas to increase the readability of the larger numbers.\n",
    "> We can do this in R by passing the `dollar_format` function \n",
    "> (from the `scales` package)\n",
    "> to the `labels` argument of the `scale_y_continuous` function.\n",
    "\n",
    "```{r 07-edaRegr, message = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Scatter plot of price (USD) versus house size (square feet).\"}\n",
    "eda <- ggplot(sacramento, aes(x = sqft, y = price)) +\n",
    "  geom_point(alpha = 0.4) +\n",
    "  xlab(\"House size (square feet)\") +\n",
    "  ylab(\"Price (USD)\") +\n",
    "  scale_y_continuous(labels = dollar_format()) + \n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "eda\n",
    "```\n",
    " \n",
    "The plot is shown in Figure \\@ref(fig:07-edaRegr).\n",
    "We can see that in Sacramento, CA, as the\n",
    "size of a house increases, so does its sale price. Thus, we can reason that we\n",
    "may be able to use the size of a not-yet-sold house (for which we don't know\n",
    "the sale price) to predict its final sale price. Note that we do not suggest here\n",
    "that a larger house size *causes* a higher sale price; just that house price\n",
    "tends to increase with house size, and that we may be able to use the latter to \n",
    "predict the former.\n",
    "\n",
    "## K-nearest neighbors regression\n",
    "\n",
    "Much like in the case of classification, \n",
    "we can use a K-nearest neighbors-based \\index{K-nearest neighbors!regression} \n",
    "approach in regression to make predictions. \n",
    "Let's take a small sample of the data in Figure \\@ref(fig:07-edaRegr) \n",
    "and walk through how K-nearest neighbors (KNN) works\n",
    "in a regression context before we dive in to creating our model and assessing\n",
    "how well it predicts house sale price. This subsample is taken to allow us to\n",
    "illustrate the mechanics of KNN regression with a few data points; later in\n",
    "this chapter we will use all the data.\n",
    "\n",
    "To take a small random sample of size 30, we'll use the function\n",
    "`slice_sample`, and input the data frame to sample from and the number of rows\n",
    "to randomly select. \\index{slice\\_sample}\n",
    "\n",
    "```{r 07-sacramento-seed, echo = FALSE, message = FALSE, warning = FALSE}\n",
    "# hidden seed\n",
    "set.seed(10)\n",
    "```\n",
    "\n",
    "```{r 07-sacramento}\n",
    "small_sacramento <- slice_sample(sacramento, n = 30)\n",
    "```\n",
    "\n",
    "Next let's say we come across a  2,000 square-foot house in Sacramento we are\n",
    "interested in purchasing, with an advertised list price of \\$350,000. Should we\n",
    "offer to pay the asking price for this house, or is it overpriced and we should\n",
    "offer less? Absent any other information, we can get a sense for a good answer\n",
    "to this question by using the data we have to predict the sale price given the\n",
    "sale prices we have already observed. But in Figure \\@ref(fig:07-small-eda-regr),\n",
    "you can see that we have no\n",
    "observations of a house of size *exactly* 2,000 square feet. How can we predict\n",
    "the sale price? \n",
    "\n",
    "```{r 07-small-eda-regr, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Scatter plot of price (USD) versus house size (square feet) with vertical line indicating 2,000 square feet on x-axis.\"}\n",
    "small_plot <- ggplot(small_sacramento, aes(x = sqft, y = price)) +\n",
    "  geom_point() +\n",
    "  xlab(\"House size (square feet)\") +\n",
    "  ylab(\"Price (USD)\") +\n",
    "  scale_y_continuous(labels = dollar_format()) +\n",
    "  geom_vline(xintercept = 2000, linetype = \"dotted\") + \n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "small_plot\n",
    "```\n",
    "\n",
    "We will employ the same intuition from the classification chapter, and use the\n",
    "neighboring points to the new point of interest to suggest/predict what its\n",
    "sale price might be. \n",
    "For the example shown in Figure \\@ref(fig:07-small-eda-regr), \n",
    "we find and label the 5 nearest neighbors to our observation \n",
    "of a house that is 2,000 square feet.\n",
    "\\index{mutate}\\index{slice}\\index{arrange}\\index{abs}\n",
    "\n",
    "```{r 07-find-k3}\n",
    "nearest_neighbors <- small_sacramento |>\n",
    "  mutate(diff = abs(2000 - sqft)) |>\n",
    "  arrange(diff) |>\n",
    "  slice(1:5) #subset the first 5 rows\n",
    "\n",
    "nearest_neighbors\n",
    "```\n",
    "\n",
    "```{r 07-knn3-example, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Scatter plot of price (USD) versus house size (square feet) with lines to 5 nearest neighbors.\"}\n",
    "nearest_neighbors <- mutate(nearest_neighbors, twothou = rep(2000, 5))\n",
    "\n",
    "nn_plot <- small_plot +\n",
    "  geom_segment(\n",
    "    data = nearest_neighbors,\n",
    "    aes(x = twothou, xend = sqft, y = price, yend = price), color = \"orange\"\n",
    "  )\n",
    "\n",
    "nn_plot\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:07-knn3-example) illustrates the difference between the house sizes\n",
    "of the 5 nearest neighbors (in terms of house size) to our new\n",
    "2,000 square-foot house of interest. Now that we have obtained these nearest neighbors, \n",
    "we can use their values to predict the\n",
    "sale price for the new home.  Specifically, we can take the mean (or\n",
    "average) of these 5 values as our predicted value, as illustrated by\n",
    "the red point in Figure \\@ref(fig:07-predictedViz-knn).\n",
    "\n",
    "```{r 07-predicted-knn}\n",
    "prediction <- nearest_neighbors |>\n",
    "  summarise(predicted = mean(price))\n",
    "\n",
    "prediction\n",
    "```\n",
    "\n",
    "```{r 07-predictedViz-knn, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Scatter plot of price (USD) versus house size (square feet) with predicted price for a 2,000 square-foot house based on 5 nearest neighbors represented as a red dot.\"}\n",
    "nn_plot +\n",
    "  geom_point(aes(x = 2000, y = prediction[[1]]), color = \"red\", size = 2.5)\n",
    "```\n",
    "\n",
    "Our predicted price is \\$`r format(round(prediction[[1]]), big.mark=\",\", nsmall=0, scientific = FALSE)`\n",
    "(shown as a red point in Figure \\@ref(fig:07-predictedViz-knn)), which is much less than \\$350,000; perhaps we\n",
    "might want to offer less than the list price at which the house is advertised.\n",
    "But this is only the very beginning of the story. We still have all the same\n",
    "unanswered questions here with KNN regression that we had with KNN\n",
    "classification: which $K$ do we choose, and is our model any good at making\n",
    "predictions? In the next few sections, we will address these questions in the\n",
    "context of KNN regression.\n",
    "\n",
    "One strength of the KNN regression algorithm \n",
    "that we would like to draw attention to at this point\n",
    "is its ability to work well with non-linear relationships\n",
    "(i.e., if the relationship is not a straight line).\n",
    "This stems from the use of nearest neighbors to predict values.\n",
    "The algorithm really has very few assumptions \n",
    "about what the data must look like for it to work.\n",
    "\n",
    "## Training, evaluating, and tuning the model\n",
    "\n",
    "As usual, \n",
    "we must start by putting some test data away in a lock box \n",
    "that we will come back to only after we choose our final model. \n",
    "Let's take care of that now. \n",
    "Note that for the remainder of the chapter \n",
    "we'll be working with the entire Sacramento data set, \n",
    "as opposed to the smaller sample of 30 points \n",
    "that we used earlier in the chapter (Figure \\@ref(fig:07-small-eda-regr)).\n",
    "\\index{training data}\n",
    "\\index{test data}\n",
    "\n",
    "```{r 07-test-train-split}\n",
    "sacramento_split <- initial_split(sacramento, prop = 0.75, strata = price)\n",
    "sacramento_train <- training(sacramento_split)\n",
    "sacramento_test <- testing(sacramento_split)\n",
    "```\n",
    "\n",
    "Next, we'll use cross-validation \\index{cross-validation} to choose $K$. In KNN classification, we used\n",
    "accuracy to see how well our predictions matched the true labels. We cannot use\n",
    "the same metric in the regression setting, since our predictions will almost never\n",
    "*exactly* match the true response variable values. Therefore in the\n",
    "context of KNN regression we will use root mean square prediction error \\index{root mean square prediction error|see{RMSPE}}\\index{RMSPE}\n",
    "(RMSPE) instead. The mathematical formula for calculating RMSPE is: \n",
    "\n",
    "$$\\text{RMSPE} = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $n$ is the number of observations,\n",
    "- $y_i$ is the observed value for the $i^\\text{th}$ observation, and\n",
    "- $\\hat{y}_i$ is the forecasted/predicted value for the $i^\\text{th}$ observation.\n",
    "\n",
    "In other words, we compute the *squared* difference between the predicted and true response \n",
    "value for each observation in our test (or validation) set, compute the average, and then finally\n",
    "take the square root. The reason we use the *squared* difference (and not just the difference)\n",
    "is that the differences can be positive or negative, i.e., we can overshoot or undershoot the true\n",
    "response value. Figure \\@ref(fig:07-verticalerrors) illustrates both positive and negative differences\n",
    "between predicted and true response values.\n",
    "So if we want to measure error&mdash;a notion of distance between our predicted and true response values&mdash;we \n",
    "want to make sure that we are only adding up positive values, with larger positive values representing larger\n",
    "mistakes.\n",
    "If the predictions are very close to the true values, then\n",
    "RMSPE will be small. If, on the other-hand, the predictions are very\n",
    "different from the true values, then RMSPE will be quite large. When we\n",
    "use cross-validation, we will choose the $K$ that gives\n",
    "us the smallest RMSPE.\n",
    "\n",
    "```{r 07-verticalerrors, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = \"Scatter plot of price (USD) versus house size (square feet) with example predictions (blue line) and the error in those predictions compared with true response values for three selected observations (vertical red lines).\", fig.height = 3.5, fig.width = 4.5}\n",
    "# save the seed\n",
    "seedval <- .Random.seed\n",
    "\n",
    "# (synthetic) new prediction points\n",
    "pts <- tibble(sqft = c(1250, 1850, 2250), price = c(250000, 200000, 500000))\n",
    "finegrid <- tibble(sqft = seq(from = 900, to = 3900, by = 10))\n",
    "\n",
    "# fit the model\n",
    "sacr_recipe_hid <- recipe(price ~ sqft, data = small_sacramento) |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors())\n",
    "\n",
    "sacr_spec_hid <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                                  neighbors = 4) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "sacr_fit_hid <- workflow() |>\n",
    "  add_recipe(sacr_recipe_hid) |>\n",
    "  add_model(sacr_spec_hid) |>\n",
    "  fit(data = small_sacramento)\n",
    "\n",
    "sacr_full_preds_hid <- sacr_fit_hid |> \n",
    "                           predict(finegrid) |>\n",
    "                           bind_cols(finegrid)\n",
    "\n",
    "sacr_new_preds_hid <- sacr_fit_hid |> \n",
    "                           predict(pts) |>\n",
    "                           bind_cols(pts)\n",
    "\n",
    "# plot the vertical prediction errors\n",
    "errors_plot <- ggplot(small_sacramento, aes(x = sqft, y = price)) +\n",
    "  geom_point() +\n",
    "  xlab(\"House size (square feet)\") +\n",
    "  ylab(\"Price (USD)\") +\n",
    "  scale_y_continuous(labels = dollar_format()) +\n",
    "  geom_line(data = sacr_full_preds_hid, \n",
    "            aes(x = sqft, y = .pred), \n",
    "            color = \"blue\") + \n",
    "  geom_segment(\n",
    "    data = sacr_new_preds_hid,\n",
    "    aes(x = sqft, xend = sqft, y = price, yend = .pred), \n",
    "    color = \"red\") + \n",
    "  geom_point(data = sacr_new_preds_hid, \n",
    "             aes(x = sqft, y = price), \n",
    "             color = \"black\")\n",
    "\n",
    "# restore the seed\n",
    ".Random.seed <- seedval\n",
    "\n",
    "errors_plot\n",
    "```\n",
    "\n",
    "> **Note:** When using many code packages (`tidymodels` included), the evaluation output \n",
    "> we will get to assess the prediction quality of\n",
    "> our KNN regression models is labeled \"RMSE\", or \"root mean squared\n",
    "> error\". Why is this so, and why not RMSPE? \\index{RMSPE!comparison with RMSE}\n",
    "> In statistics, we try to be very precise with our\n",
    "> language to indicate whether we are calculating the prediction error on the\n",
    "> training data (*in-sample* prediction) versus on the testing data \n",
    "> (*out-of-sample* prediction). When predicting and evaluating prediction quality on the training data, we \n",
    ">  say RMSE. By contrast, when predicting and evaluating prediction quality\n",
    "> on the testing or validation data, we say RMSPE. \n",
    "> The equation for calculating RMSE and RMSPE is exactly the same; all that changes is whether the $y$s are\n",
    "> training or testing data. But many people just use RMSE for both, \n",
    "> and rely on context to denote which data the root mean squared error is being calculated on.\n",
    "\n",
    "Now that we know how we can assess how well our model predicts a numerical\n",
    "value, let's use R to perform cross-validation and to choose the optimal $K$.\n",
    "First, we will create a recipe for preprocessing our data.\n",
    "Note that we include standardization\n",
    "in our preprocessing to build good habits, but since we only have one \n",
    "predictor, it is technically not necessary; there is no risk of comparing two predictors\n",
    "of different scales.\n",
    "Next we create a model specification for K-nearest neighbors regression. Note \n",
    "that we use `set_mode(\"regression\")`\n",
    "now in the model specification to denote a regression problem, as opposed to the classification\n",
    "problems from the previous chapters. \n",
    "The use of `set_mode(\"regression\")` essentially\n",
    " tells `tidymodels` that we need to use different metrics (RMSPE, not accuracy)\n",
    "for tuning and evaluation.\n",
    "Then we create a 5-fold cross-validation object, and put the recipe and model specification together\n",
    "in a workflow.\n",
    "\\index{tidymodels}\\index{recipe}\\index{workflow}\n",
    "\n",
    "```{r 07-choose-k-knn, results = 'hide', echo = TRUE}\n",
    "sacr_recipe <- recipe(price ~ sqft, data = sacramento_train) |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors())\n",
    "\n",
    "sacr_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                              neighbors = tune()) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "sacr_vfold <- vfold_cv(sacramento_train, v = 5, strata = price)\n",
    "\n",
    "sacr_wkflw <- workflow() |>\n",
    "  add_recipe(sacr_recipe) |>\n",
    "  add_model(sacr_spec)\n",
    "\n",
    "sacr_wkflw\n",
    "```\n",
    "\n",
    "```{r echo = FALSE}\n",
    "print_tidymodels(sacr_wkflw)\n",
    "```\n",
    "\n",
    "Next we run cross-validation for a grid of numbers of neighbors ranging from 1 to 200. \n",
    "The following code tunes\n",
    "the model and returns the RMSPE for each number of neighbors. In the output of the `sacr_results`\n",
    "results data frame, we see that the `neighbors` variable contains the value of $K$,\n",
    "the mean (`mean`) contains the value of the RMSPE estimated via cross-validation,\n",
    "and the standard error (`std_err`) contains a value corresponding to a measure of how uncertain we are in the mean value. A detailed treatment of this\n",
    "is beyond the scope of this chapter; but roughly, if your estimated mean is 100,000 and standard\n",
    "error is 1,000, you can expect the *true* RMSPE to be somewhere roughly between 99,000 and 101,000 (although it may\n",
    "fall outside this range). You may ignore the other columns in the metrics data frame,\n",
    "as they do not provide any additional insight.\n",
    "\\index{cross-validation!collect\\_metrics}\n",
    "\n",
    "```{r 07-choose-k-knn-results}\n",
    "gridvals <- tibble(neighbors = seq(from = 1, to = 200, by = 3))\n",
    "\n",
    "sacr_results <- sacr_wkflw |>\n",
    "  tune_grid(resamples = sacr_vfold, grid = gridvals) |>\n",
    "  collect_metrics() |>\n",
    "  filter(.metric == \"rmse\")\n",
    "\n",
    "# show the results\n",
    "sacr_results\n",
    "```\n",
    "\n",
    "```{r 07-choose-k-knn-plot, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Effect of the number of neighbors on the RMSPE.\"} \n",
    "sacr_tunek_plot <- ggplot(sacr_results, aes(x = neighbors, y = mean)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  labs(x = \"Neighbors\", y = \"RMSPE\")\n",
    "\n",
    "sacr_tunek_plot\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:07-choose-k-knn-plot) visualizes how the RMSPE varies with the number of neighbors $K$.\n",
    "We take the *minimum* RMSPE to find the best setting for the number of neighbors:\n",
    "\n",
    "```{r 07-choose-k-knn-results-2}\n",
    "# show only the row of minimum RMSPE\n",
    "sacr_min <- sacr_results |>\n",
    "  filter(mean == min(mean))\n",
    "\n",
    "sacr_min\n",
    "```\n",
    "\n",
    "The smallest RMSPE occurs when $K =$ `r sacr_min |> pull(neighbors)`.\n",
    "\n",
    "```{r 07-get-kmin, echo = FALSE, message = FALSE, warning = FALSE}\n",
    "kmin <- sacr_min |> pull(neighbors)\n",
    "```\n",
    "\n",
    "## Underfitting and overfitting\n",
    "Similar to the setting of classification, by setting the number of neighbors\n",
    "to be too small or too large, we cause the RMSPE to increase, as shown in\n",
    "Figure \\@ref(fig:07-choose-k-knn-plot). What is happening here? \n",
    "\n",
    "Figure \\@ref(fig:07-howK) visualizes the effect of different settings of $K$ on the\n",
    "regression model. Each plot shows the predicted values for house sale price from\n",
    "our KNN regression model for 6 different values for $K$: 1, 3, `r kmin`, 41, 250, and 932 (i.e., the entire dataset).\n",
    "For each model, we predict prices for the range of possible home sizes we\n",
    "observed in the data set (here 500 to 5,000 square feet) and we plot the\n",
    "predicted prices as a blue line.\n",
    "\n",
    "```{r 07-howK, echo = FALSE, warning = FALSE, fig.height = 13, fig.width = 10,fig.cap = \"Predicted values for house price (represented as a blue line) from KNN regression models for six different values for $K$.\"}\n",
    "gridvals <- c(1, 3, kmin, 41, 250, 932)\n",
    "\n",
    "plots <- list()\n",
    "\n",
    "for (i in 1:6) {\n",
    "  if (i < 6){\n",
    "      sacr_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                                    neighbors = gridvals[[i]]) |>\n",
    "        set_engine(\"kknn\") |>\n",
    "        set_mode(\"regression\")\n",
    "\n",
    "      sacr_wkflw <- workflow() |>\n",
    "        add_recipe(sacr_recipe) |>\n",
    "        add_model(sacr_spec)\n",
    "\n",
    "      sacr_preds <- sacr_wkflw |>\n",
    "        fit(data = sacramento_train) |>\n",
    "        predict(sacramento_train) |>\n",
    "        bind_cols(sacramento_train)\n",
    "\n",
    "      plots[[i]] <- ggplot(sacr_preds, aes(x = sqft, y = price)) +\n",
    "        geom_point(alpha = 0.4) +\n",
    "        xlab(\"House size (square feet)\") +\n",
    "        ylab(\"Price (USD)\") +\n",
    "        scale_y_continuous(labels = dollar_format()) +\n",
    "        geom_line(data = sacr_preds, aes(x = sqft, y = .pred), color = \"blue\") +\n",
    "        ggtitle(paste0(\"K = \", gridvals[[i]])) +\n",
    "        theme(text = element_text(size = 20), axis.title=element_text(size=20))\n",
    "      } else {\n",
    "      plots[[i]] <- ggplot(sacr_preds, aes(x = sqft, y = price)) +\n",
    "        geom_point(alpha = 0.4) +\n",
    "        xlab(\"House size (square feet)\") +\n",
    "        ylab(\"Price (USD)\") +\n",
    "        scale_y_continuous(labels = dollar_format()) +\n",
    "        geom_hline(data = sacr_preds, \n",
    "                   mapping = aes(x = sqft), \n",
    "                   yintercept = mean(sacr_preds$price), \n",
    "                   color = \"blue\") +\n",
    "        ggtitle(paste0(\"K = \", gridvals[[i]])) +\n",
    "  theme(text = element_text(size = 20), axis.title=element_text(size=20)) \n",
    "  }\n",
    "}\n",
    "\n",
    "grid.arrange(grobs = plots, ncol = 2)\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:07-howK) shows that when $K$ = 1, the blue line runs perfectly\n",
    "through (almost) all of our training observations. \n",
    "This happens because our\n",
    "predicted values for a given region (typically) depend on just a single observation. \n",
    "In general, when $K$ is too small, the line follows the training data quite\n",
    "closely, even if it does not match it perfectly.\n",
    "If we used a different training data set of house prices and sizes\n",
    "from the Sacramento real estate market, we would end up with completely different\n",
    "predictions. In other words, the model is *influenced too much* by the data.\n",
    "Because the model follows the training data so closely, it will not make accurate\n",
    "predictions on new observations which, generally, will not have the same fluctuations\n",
    "as the original training data.\n",
    "Recall from the classification\n",
    "chapters that this behavior&mdash;where the model is influenced too much\n",
    "by the noisy data&mdash;is called *overfitting*; we use this same term \n",
    "in the context of regression. \\index{overfitting!regression}\n",
    "\n",
    "What about the plots in Figure \\@ref(fig:07-howK) where $K$ is quite large, \n",
    "say, $K$ = 250 or 932? \n",
    "In this case the blue line becomes extremely smooth, and actually becomes flat\n",
    "once $K$ is equal to the number of datapoints in the entire data set. \n",
    "This happens because our predicted values for a given x value (here, home\n",
    "size), depend on many neighboring observations; in the case where $K$ is equal \n",
    "to the size of the dataset, the prediction is just the mean of the house prices\n",
    "in the dataset (completely ignoring the house size). \n",
    "In contrast to the $K=1$ example, \n",
    "the smooth, inflexible blue line does not follow the training observations very closely.\n",
    "In other words, the model is *not influenced enough* by the training data.\n",
    "Recall from the classification\n",
    "chapters that this behavior is called *underfitting*; we again use this same\n",
    "term in the context of regression.  \\index{underfitting!regression}\n",
    "\n",
    "Ideally, what we want is neither of the two situations discussed above. Instead,\n",
    "we would like a model that (1) follows the overall \"trend\" in the training data, so the model\n",
    "actually uses the training data to learn something useful, and (2) does not follow\n",
    "the noisy fluctuations, so that we can be confident that our model will transfer/generalize\n",
    "well to other new data. If we explore \n",
    "the other values for $K$, in particular $K$ = `r sacr_min |> pull(neighbors)`\n",
    "(as suggested by cross-validation),\n",
    "we can see it achieves this goal: it follows the increasing trend of house price\n",
    "versus house size, but is not influenced too much by the idiosyncratic variations\n",
    "in price. All of this is similar to how\n",
    "the choice of $K$ affects K-nearest neighbors classification, as discussed in the previous\n",
    "chapter. \n",
    "\n",
    "## Evaluating on the test set\n",
    "\n",
    "To assess how well our model might do at predicting on unseen data, we will\n",
    "assess its RMSPE on the test data. To do this, we will first\n",
    "re-train our KNN regression model on the entire training data set, \n",
    "using $K =$ `r sacr_min |> pull(neighbors)` neighbors. Then we will\n",
    "use `predict` to make predictions on the test data, and use the `metrics`\n",
    "function again to compute the summary of regression quality. Because\n",
    "we specify that we are performing regression in `set_mode`, the `metrics`\n",
    "function knows to output a quality summary related to regression, and not, say, classification.\n",
    "\n",
    "```{r 07-predict}\n",
    "kmin <- sacr_min |> pull(neighbors)\n",
    "\n",
    "sacr_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = kmin) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "sacr_fit <- workflow() |>\n",
    "  add_recipe(sacr_recipe) |>\n",
    "  add_model(sacr_spec) |>\n",
    "  fit(data = sacramento_train)\n",
    "\n",
    "sacr_summary <- sacr_fit |>\n",
    "  predict(sacramento_test) |>\n",
    "  bind_cols(sacramento_test) |>\n",
    "  metrics(truth = price, estimate = .pred) |>\n",
    "  filter(.metric == 'rmse')\n",
    "\n",
    "sacr_summary\n",
    "```\n",
    "\n",
    "Our final model's test error as assessed by RMSPE \n",
    "is $`r format(round(sacr_summary |> pull(.estimate)), big.mark=\",\", nsmall=0, scientific=FALSE)`. \n",
    "Note that RMSPE is measured in the same units as the response variable.\n",
    "In other words, on new observations, we expect the error in our prediction to be \n",
    "*roughly* $`r format(round(sacr_summary |> pull(.estimate)), big.mark=\",\", nsmall=0, scientific=FALSE)`. \n",
    "From one perspective, this is good news: this is about the same as the cross-validation\n",
    "RMSPE estimate of our tuned model \n",
    "(which was $`r format(round(sacr_min |> pull(mean)), big.mark=\",\", nsmall=0, scientific=FALSE)`), \n",
    "so we can say that the model appears to generalize well\n",
    "to new data that it has never seen before.\n",
    "However, much like in the case of KNN classification, whether this value for RMSPE is *good*&mdash;i.e.,\n",
    "whether an error of around $`r format(round(sacr_summary |> pull(.estimate)), big.mark=\",\", nsmall=0, scientific=FALSE)`\n",
    "is acceptable&mdash;depends entirely on the application. \n",
    "In this application, this error\n",
    "is not prohibitively large, but it is not negligible either; \n",
    "$`r format(round(sacr_summary |> pull(.estimate)), big.mark=\",\", nsmall=0, scientific=FALSE)`\n",
    "might represent a substantial fraction of a home buyer's budget, and\n",
    "could make or break whether or not they could afford put an offer on a house. \n",
    "\n",
    "Finally, Figure \\@ref(fig:07-predict-all) shows the predictions that our final model makes across\n",
    "the range of house sizes we might encounter in the Sacramento area&mdash;from 500 to 5000 square feet. \n",
    "You have already seen a few plots like this in this chapter, but here we also provide the code that generated it\n",
    "as a learning challenge.\n",
    "\n",
    "```{r 07-predict-all, warning = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Predicted values of house price (blue line) for the final KNN regression model.\"}\n",
    "sacr_preds <- tibble(sqft = seq(from = 500, to = 5000, by = 10))\n",
    "\n",
    "sacr_preds <- sacr_fit |>\n",
    "  predict(sacr_preds) |>\n",
    "  bind_cols(sacr_preds)\n",
    "\n",
    "plot_final <- ggplot(sacramento_train, aes(x = sqft, y = price)) +\n",
    "  geom_point(alpha = 0.4) +\n",
    "  geom_line(data = sacr_preds, \n",
    "            mapping = aes(x = sqft, y = .pred), \n",
    "            color = \"blue\") +\n",
    "  xlab(\"House size (square feet)\") +\n",
    "  ylab(\"Price (USD)\") +\n",
    "  scale_y_continuous(labels = dollar_format()) +\n",
    "  ggtitle(paste0(\"K = \", kmin)) + \n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "plot_final\n",
    "```\n",
    "\n",
    "## Multivariable KNN regression\n",
    "\n",
    "As in KNN classification, we can use multiple predictors in KNN regression.\n",
    "In this setting, we have the same concerns regarding the scale of the predictors. Once again, \n",
    " predictions are made by identifying the $K$\n",
    "observations that are nearest to the new point we want to predict; any\n",
    "variables that are on a large scale will have a much larger effect than\n",
    "variables on a small scale. But since the `recipe` we built above scales and centers\n",
    "all predictor variables, this is handled for us.\n",
    "\n",
    "Note that we also have the same concern regarding the selection of predictors\n",
    "in KNN regression as in KNN classification: having more predictors is **not** always\n",
    "better, and the choice of which predictors to use has a potentially large influence\n",
    "on the quality of predictions. Fortunately, we can use the predictor selection \n",
    "algorithm from the classification chapter in KNN regression as well.\n",
    "As the algorithm is the same, we will not cover it again in this chapter.\n",
    "\n",
    "We will now demonstrate a multivariable KNN regression \\index{K-nearest neighbors!multivariable regression}  analysis of the \n",
    "Sacramento real estate \\index{Sacramento real estate} data using `tidymodels`. This time we will use\n",
    "house size (measured in square feet) as well as number of bedrooms as our\n",
    "predictors, and continue to use house sale price as our outcome/target variable\n",
    "that we are trying to predict.\n",
    "It is always a good practice to do exploratory data analysis, such as\n",
    "visualizing the data, before we start modeling the data. Figure \\@ref(fig:07-bedscatter)\n",
    "shows that the number of bedrooms might provide useful information\n",
    "to help predict the sale price of a house.\n",
    "\n",
    "```{r 07-bedscatter, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Scatter plot of the sale price of houses versus the number of bedrooms.\"}\n",
    "plot_beds <- sacramento |>\n",
    "  ggplot(aes(x = beds, y = price)) +\n",
    "  geom_point(alpha = 0.4) + \n",
    "  labs(x = 'Number of Bedrooms', y = 'Price (USD)') + \n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "plot_beds\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:07-bedscatter) shows that as the number of bedrooms increases,\n",
    "the house sale price tends to increase as well, but that the relationship\n",
    "is quite weak. Does adding the number of bedrooms\n",
    "to our model improve our ability to predict price? To answer that\n",
    "question, we will have to create a new KNN regression\n",
    "model using house size and number of bedrooms, and then we can compare it to\n",
    "the model we previously came up with that only used house\n",
    "size. Let's do that now!\n",
    "\n",
    "First we'll build a new model specification and recipe for the analysis. Note that\n",
    "we use the formula `price ~ sqft + beds` to denote that we have two predictors,\n",
    "and set `neighbors = tune()` to tell `tidymodels` to tune the number of neighbors for us.\n",
    "\n",
    "```{r 07-mult-setup}\n",
    "sacr_recipe <- recipe(price ~ sqft + beds, data = sacramento_train) |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors())\n",
    "\n",
    "sacr_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                              neighbors = tune()) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"regression\")\n",
    "```\n",
    "\n",
    "Next, we'll use 5-fold cross-validation to choose the number of neighbors via the minimum RMSPE:\n",
    "\n",
    "```{r 07-mult-cv}\n",
    "gridvals <- tibble(neighbors = seq(1, 200))\n",
    "\n",
    "sacr_multi <- workflow() |>\n",
    "  add_recipe(sacr_recipe) |>\n",
    "  add_model(sacr_spec) |>\n",
    "  tune_grid(sacr_vfold, grid = gridvals) |>\n",
    "  collect_metrics() |>\n",
    "  filter(.metric == \"rmse\") |>\n",
    "  filter(mean == min(mean))\n",
    "\n",
    "sacr_k <- sacr_multi |>\n",
    "              pull(neighbors)\n",
    "\n",
    "sacr_multi\n",
    "```\n",
    "\n",
    "Here we see that the smallest estimated RMSPE from cross-validation occurs when $K =$ `r sacr_k`.\n",
    "If we want to compare this multivariable KNN regression model to the model with only a single\n",
    "predictor *as part of the model tuning process* (e.g., if we are running forward selection as described\n",
    "in the chapter on evaluating and tuning classification models),\n",
    "then we must compare the accuracy estimated using only the training data via cross-validation.\n",
    "Looking back, the estimated cross-validation accuracy for the single-predictor \n",
    "model was `r format(round(sacr_min$mean), big.mark=\",\", nsmall=0, scientific = FALSE)`.\n",
    "The estimated cross-validation accuracy for the multivariable model is\n",
    "`r format(round(sacr_multi$mean), big.mark=\",\", nsmall=0, scientific = FALSE)`.\n",
    "Thus in this case, we did not improve the model \n",
    "by a large amount by adding this additional predictor.\n",
    "\n",
    "Regardless, let's continue the analysis to see how we can make predictions with a multivariable KNN regression model\n",
    "and evaluate its performance on test data. We first need to re-train the model on the entire\n",
    "training data set with $K =$ `r sacr_k`, and then use that model to make predictions\n",
    "on the test data.\n",
    "\n",
    "```{r 07-re-train}\n",
    "sacr_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                              neighbors = sacr_k) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"regression\")\n",
    "\n",
    "knn_mult_fit <- workflow() |>\n",
    "  add_recipe(sacr_recipe) |>\n",
    "  add_model(sacr_spec) |>\n",
    "  fit(data = sacramento_train)\n",
    "\n",
    "knn_mult_preds <- knn_mult_fit |>\n",
    "  predict(sacramento_test) |>\n",
    "  bind_cols(sacramento_test)\n",
    "\n",
    "knn_mult_mets <- metrics(knn_mult_preds, truth = price, estimate = .pred) |>\n",
    "                     filter(.metric == 'rmse')\n",
    "knn_mult_mets\n",
    "```\n",
    "\n",
    "This time, when we performed KNN regression on the same data set, but also\n",
    "included number of bedrooms as a predictor, we obtained a RMSPE test error \n",
    "of `r format(round(knn_mult_mets |> pull(.estimate)), big.mark=\",\", nsmall=0, scientific=FALSE)`.\n",
    "Figure \\@ref(fig:07-knn-mult-viz) visualizes the model's predictions overlaid on top of the data. This \n",
    "time the predictions are a surface in 3D space, instead of a line in 2D space, as we have 2\n",
    "predictors instead of 1.  \n",
    "\n",
    "```{r 07-knn-mult-viz, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = \"KNN regression model’s predictions represented as a surface in 3D space overlaid on top of the data using three predictors (price, house size, and the number of bedrooms). Note that in general we recommend against using 3D visualizations; here we use a 3D visualization only to illustrate what the surface of predictions looks like for learning purposes.\", out.width=\"100%\"}\n",
    "xvals <- seq(from = min(sacramento_train$sqft), \n",
    "             to = max(sacramento_train$sqft), \n",
    "             length = 50)\n",
    "yvals <- seq(from = min(sacramento_train$beds), \n",
    "             to = max(sacramento_train$beds), \n",
    "             length = 50)\n",
    "\n",
    "zvals <- knn_mult_fit |>\n",
    "  predict(crossing(xvals, yvals) |> \n",
    "            mutate(sqft = xvals, beds = yvals)) |>\n",
    "  pull(.pred)\n",
    "\n",
    "zvalsm <- matrix(zvals, nrow = length(xvals))\n",
    "\n",
    "plot_3d <- plot_ly() |>\n",
    "  add_markers(\n",
    "    data = sacramento_train,\n",
    "    x = ~sqft,\n",
    "    y = ~beds,\n",
    "    z = ~price,\n",
    "    marker = list(size = 2, opacity = 0.4, color = \"red\")\n",
    "  ) |>\n",
    "  layout(scene = list(\n",
    "    xaxis = list(title = \"Size (sq ft)\"),\n",
    "    zaxis = list(title = \"Price (USD)\"),\n",
    "    yaxis = list(title = \"Bedrooms\")\n",
    "  )) |>\n",
    "  add_surface(\n",
    "    x = ~xvals,\n",
    "    y = ~yvals,\n",
    "    z = ~zvalsm,\n",
    "    colorbar = list(title = \"Price (USD)\")\n",
    "  )\n",
    "\n",
    "if(!is_latex_output()){  \n",
    "  plot_3d\n",
    "} else {\n",
    "  scene = list(camera = list(eye = list(x = -2.1, y = -2.2, z = 0.75)))\n",
    "  plot_3d <- plot_3d  |> layout(scene = scene)\n",
    "  save_image(plot_3d, \"img/plot3d_knn_regression.png\", scale = 10)\n",
    "  knitr::include_graphics(\"img/plot3d_knn_regression.png\")\n",
    "}\n",
    "```\n",
    "\n",
    "We can see that the predictions in this case, where we have 2 predictors, form\n",
    "a surface instead of a line. Because the newly added predictor (number of bedrooms) is \n",
    "related to price (as price changes, so does number of bedrooms)\n",
    "and is not totally determined by house size (our other predictor),\n",
    "we get additional and useful information for making our\n",
    "predictions. For example, in this model we would predict that the cost of a\n",
    "house with a size of 2,500 square feet generally increases slightly as the number\n",
    "of bedrooms increases. Without having the additional predictor of number of\n",
    "bedrooms, we would predict the same price for these two houses.\n",
    "\n",
    "## Strengths and limitations of KNN regression\n",
    "\n",
    "As with KNN classification (or any prediction algorithm for that matter), KNN \n",
    "regression has both strengths and weaknesses. Some are listed here:\n",
    "\n",
    "**Strengths:** K-nearest neighbors regression\n",
    "\n",
    "1. is a simple, intuitive algorithm,\n",
    "2. requires few assumptions about what the data must look like, and \n",
    "3. works well with non-linear relationships (i.e., if the relationship is not a straight line).\n",
    "\n",
    "**Weaknesses:** K-nearest neighbors regression\n",
    "\n",
    "1. becomes very slow as the training data gets larger,\n",
    "2. may not perform well with a large number of predictors, and\n",
    "3. may not predict well beyond the range of values input in your training data.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Practice exercises for the material covered in this chapter \n",
    "can be found in the accompanying \n",
    "[worksheets repository](https://github.com/UBC-DSCI/data-science-a-first-intro-worksheets#readme)\n",
    "in the \"Regression I: K-nearest neighbors\" row.\n",
    "You can launch an interactive version of the worksheet in your browser by clicking the \"launch binder\" button.\n",
    "You can also preview a non-interactive version of the worksheet by clicking \"view worksheet.\"\n",
    "If you instead decide to download the worksheet and run it on your own machine,\n",
    "make sure to follow the instructions for computer setup\n",
    "found in Chapter \\@ref(move-to-your-own-machine). This will ensure that the automated feedback\n",
    "and guidance that the worksheets provide will function as intended."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,md:myst,ipynb",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.8"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}