{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a917805f",
   "metadata": {},
   "source": [
    "# Classification I: training & predicting {#classification}\n",
    "\n",
    "```{r classification1-setup, echo = FALSE, message = FALSE, warning = FALSE}\n",
    "library(formatR)\n",
    "library(plotly)\n",
    "library(knitr)\n",
    "library(kableExtra)\n",
    "library(ggpubr)\n",
    "library(stringr)\n",
    "library(ggplot2)\n",
    "\n",
    "knitr::opts_chunk$set(echo = TRUE, \n",
    "                      fig.align = \"center\")\n",
    "options(knitr.table.format = function() {\n",
    "  if (knitr::is_latex_output()) 'latex' else 'pandoc'\n",
    "})\n",
    "reticulate::use_miniconda('r-reticulate')\n",
    "\n",
    "print_tidymodels <- function(tidymodels_object) {\n",
    "  if(!is_latex_output()) {\n",
    "    tidymodels_object\n",
    "  } else {\n",
    "    output <- capture.output(tidymodels_object)\n",
    "    \n",
    "    for (i in seq_along(output)) {\n",
    "      if (nchar(output[i]) <= 80) {\n",
    "        cat(output[i], sep = \"\\n\")\n",
    "      } else {\n",
    "        cat(str_sub(output[i], start = 1, end = 80), sep = \"\\n\")\n",
    "        cat(str_sub(output[i], start = 81, end = nchar(output[i])), sep = \"\\n\")\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "theme_update(axis.title = element_text(size = 12)) # modify axis label size in plots \n",
    "```\n",
    "\n",
    "## Overview \n",
    "In previous chapters, we focused solely on descriptive and exploratory\n",
    "data analysis questions. \n",
    "This chapter and the next together serve as our first\n",
    "foray into answering *predictive* questions about data. In particular, we will\n",
    "focus on *classification*, i.e., using one or more \n",
    "variables to predict the value of a categorical variable of interest. This chapter\n",
    "will cover the basics of classification, how to preprocess data to make it\n",
    "suitable for use in a classifier, and how to use our observed data to make\n",
    "predictions. The next chapter will focus on how to evaluate how accurate the\n",
    "predictions from our classifier are, as well as how to improve our classifier\n",
    "(where possible) to maximize its accuracy.\n",
    "\n",
    "## Chapter learning objectives \n",
    "\n",
    "By the end of the chapter, readers will be able to do the following:\n",
    "\n",
    "- Recognize situations where a classifier would be appropriate for making predictions.\n",
    "- Describe what a training data set is and how it is used in classification.\n",
    "- Interpret the output of a classifier.\n",
    "- Compute, by hand, the straight-line (Euclidean) distance between points on a graph when there are two predictor variables.\n",
    "- Explain the $K$-nearest neighbor classification algorithm.\n",
    "- Perform $K$-nearest neighbor classification in R using `tidymodels`.\n",
    "- Use a `recipe` to preprocess data to be centered, scaled, and balanced.\n",
    "- Combine preprocessing and model training using a `workflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde5cea",
   "metadata": {},
   "source": [
    "## The classification problem\n",
    "In many situations, we want to make predictions \\index{predictive question} based on the current situation\n",
    "as well as past experiences. For instance, a doctor may want to diagnose a\n",
    "patient as either diseased or healthy based on their symptoms and the doctor's\n",
    "past experience with patients; an email provider might want to tag a given\n",
    "email as \"spam\" or \"not spam\" based on the email's text and past email text data; \n",
    "or a credit card company may want to predict whether a purchase is fraudulent based\n",
    "on the current purchase item, amount, and location as well as past purchases.\n",
    "These tasks are all examples of \\index{classification} **classification**, i.e., predicting a\n",
    "categorical class (sometimes called a *label*) \\index{class}\\index{categorical variable} for an observation given its\n",
    "other variables (sometimes called *features*). \\index{feature|see{predictor}}\n",
    "\n",
    "Generally, a classifier assigns an observation without a known class (e.g., a new patient) \n",
    "to a class (e.g., diseased or healthy) on the basis of how similar it is to other observations\n",
    "for which we do know the class (e.g., previous patients with known diseases and\n",
    "symptoms). These observations with known classes that we use as a basis for\n",
    "prediction are called a **training set**; \\index{training set} this name comes from the fact that\n",
    "we use these data to train, or teach, our classifier. Once taught, we can use\n",
    "the classifier to make predictions on new data for which we do not know the class.\n",
    "\n",
    "There are many possible methods that we could use to predict\n",
    "a categorical class/label for an observation. In this book, we will\n",
    "focus on the widely used **$K$-nearest neighbors** \\index{K-nearest neighbors} algorithm [@knnfix; @knncover].\n",
    "In your future studies, you might encounter decision trees, support vector machines (SVMs),\n",
    "logistic regression, neural networks, and more; see the additional resources\n",
    "section at the end of the next chapter for where to begin learning more about\n",
    "these other methods. It is also worth mentioning that there are many\n",
    "variations on the basic classification problem. For example, \n",
    "we focus on the setting of **binary classification** \\index{classification!binary} where only two\n",
    "classes are involved (e.g., a diagnosis of either healthy or diseased), but you may \n",
    "also run into multiclass classification problems with more than two\n",
    "categories (e.g., a diagnosis of healthy, bronchitis, pneumonia, or a common cold).\n",
    "\n",
    "## Exploring a data set\n",
    "\n",
    "In this chapter and the next, we will study a data set of \n",
    "[digitized breast cancer image features](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29),\n",
    "created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian [@streetbreastcancer]. \\index{breast cancer} \n",
    "Each row in the data set represents an\n",
    "image of a tumor sample, including the diagnosis (benign or malignant) and\n",
    "several other measurements (nucleus texture, perimeter, area, and more).\n",
    "Diagnosis for each image was conducted by physicians. \n",
    "\n",
    "As with all data analyses, we first need to formulate a precise question that\n",
    "we want to answer. Here, the question is *predictive*: \\index{question!classification} can \n",
    "we use the tumor\n",
    "image measurements available to us to predict whether a future tumor image\n",
    "(with unknown diagnosis) shows a benign or malignant tumor? Answering this\n",
    "question is important because traditional, non-data-driven methods for tumor\n",
    "diagnosis are quite subjective and dependent upon how skilled and experienced\n",
    "the diagnosing physician is. Furthermore, benign tumors are not normally\n",
    "dangerous; the cells stay in the same place, and the tumor stops growing before\n",
    "it gets very large. By contrast, in malignant tumors, the cells invade the\n",
    "surrounding tissue and spread into nearby organs, where they can cause serious\n",
    "damage [@stanfordhealthcare].\n",
    "Thus, it is important to quickly and accurately diagnose the tumor type to\n",
    "guide patient treatment.\n",
    "\n",
    "### Loading the cancer data\n",
    "\n",
    "Our first step is to load, wrangle, and explore the data using visualizations\n",
    "in order to better understand the data we are working with. We start by\n",
    "loading the `tidyverse` package needed for our analysis. \n",
    "\n",
    "```{r 05-load-libraries, warning = FALSE, message = FALSE}\n",
    "library(tidyverse)\n",
    "```\n",
    "\n",
    "In this case, the file containing the breast cancer data set is a `.csv`\n",
    "file with headers. We'll use the `read_csv` function with no additional\n",
    "arguments, and then inspect its contents:\n",
    "\n",
    "\\index{read function!read\\_csv}\n",
    "\n",
    "```{r 05-read-data, message = FALSE}\n",
    "cancer <- read_csv(\"data/wdbc.csv\")\n",
    "cancer\n",
    "```\n",
    "\n",
    "### Describing the variables in the cancer data set\n",
    "\n",
    "Breast tumors can be diagnosed by performing a *biopsy*, a process where\n",
    "tissue is removed from the body and examined for the presence of disease.\n",
    "Traditionally these procedures were quite invasive; modern methods such as fine\n",
    "needle aspiration, used to collect the present data set, extract only a small\n",
    "amount of tissue and are less invasive. Based on a digital image of each breast\n",
    "tissue sample collected for this data set, ten different variables were measured\n",
    "for each cell nucleus in the image (items 3&ndash;12 of the list of variables below), and then the mean \n",
    " for each variable across the nuclei was recorded. As part of the\n",
    "data preparation, these values have been *standardized (centered and scaled)*; we will discuss what this\n",
    "means and why we do it later in this chapter. Each image additionally was given\n",
    "a unique ID and a diagnosis by a physician.  Therefore, the\n",
    "total set of variables per image in this data set is:\n",
    "\n",
    "1. ID: identification number \n",
    "2. Class: the diagnosis (M = malignant or B = benign)\n",
    "3. Radius: the mean of distances from center to points on the perimeter\n",
    "4. Texture: the standard deviation of gray-scale values\n",
    "5. Perimeter: the length of the surrounding contour \n",
    "6. Area: the area inside the contour\n",
    "7. Smoothness: the local variation in radius lengths\n",
    "8. Compactness: the ratio of squared perimeter and area\n",
    "9. Concavity: severity of concave portions of the contour \n",
    "10. Concave Points: the number of concave portions of the contour\n",
    "11. Symmetry: how similar the nucleus is when mirrored \n",
    "12. Fractal Dimension: a measurement of how \"rough\" the perimeter is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8be9c2",
   "metadata": {},
   "source": [
    "Below we use `glimpse` \\index{glimpse} to preview the data frame. This function can \n",
    "make it easier to inspect the data when we have a lot of columns, \n",
    "as it prints the data such that the columns go down \n",
    "the page (instead of across).\n",
    "\n",
    "```{r 05-glimpse}\n",
    "glimpse(cancer)\n",
    "```\n",
    "\n",
    "From the summary of the data above, we can see that `Class` is of type character\n",
    "(denoted by `<chr>`). Since we will be working with `Class` as a\n",
    "categorical statistical variable, we will convert it to a factor using the\n",
    "function `as_factor`. \\index{factor!as\\_factor}\n",
    "\n",
    "```{r 05-class}\n",
    "cancer <- cancer |>\n",
    "  mutate(Class = as_factor(Class))\n",
    "glimpse(cancer)\n",
    "```\n",
    "\n",
    "Recall that factors have what are called \"levels\", which you can think of as categories. We\n",
    "can verify the levels of the `Class` column by using the `levels` \\index{levels}\\index{factor!levels} function.\n",
    "This function should return the name of each category in that column. Given\n",
    "that we only have two different values in our `Class` column (B for benign and M \n",
    "for malignant), we only expect to get two names back.  Note that the `levels` function requires a *vector* argument; \n",
    "so we use the `pull` function to extract a single column (`Class`) and \n",
    "pass that into the `levels` function to see the categories \n",
    "in the `Class` column. \n",
    "\n",
    "```{r 05-levels}\n",
    "cancer |>\n",
    "  pull(Class) |>\n",
    "  levels()\n",
    "```\n",
    "\n",
    "### Exploring the cancer data\n",
    "\n",
    "Before we start doing any modeling, let's explore our data set. Below we use\n",
    "the `group_by`, `summarize` and `n` \\index{group\\_by}\\index{summarize} functions to find the number and percentage \n",
    "of benign and malignant tumor observations in our data set. The `n` function within\n",
    "`summarize`, when paired with `group_by`, counts the number of observations in each `Class` group. \n",
    "Then we calculate the percentage in each group by dividing by the total number of observations. We have 357 (63\\%) benign and 212 (37\\%) malignant tumor observations.\n",
    "```{r 05-tally}\n",
    "num_obs <- nrow(cancer)\n",
    "cancer |>\n",
    "  group_by(Class) |>\n",
    "  summarize(\n",
    "    count = n(),\n",
    "    percentage = n() / num_obs * 100\n",
    "  )\n",
    "```\n",
    "\n",
    "Next, let's draw a scatter plot \\index{visualization!scatter} to visualize the relationship between the\n",
    "perimeter and concavity variables. Rather than use `ggplot's` default palette,\n",
    "we select our own colorblind-friendly colors&mdash;`\"orange2\"` \n",
    "for light orange and `\"steelblue2\"` for light blue&mdash;and\n",
    " pass them as the `values` argument to the `scale_color_manual` function. \n",
    "We also make the category labels (\"B\" and \"M\") more readable by \n",
    "changing them to \"Benign\" and \"Malignant\" using the `labels` argument.\n",
    "\n",
    "```{r 05-scatter, fig.height = 3.5, fig.width = 4.5, fig.cap= \"Scatter plot of concavity versus perimeter colored by diagnosis label.\"}\n",
    "perim_concav <- cancer |>\n",
    "  ggplot(aes(x = Perimeter, y = Concavity, color = Class)) +\n",
    "  geom_point(alpha = 0.6) +\n",
    "  labs(x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\",\n",
    "       color = \"Diagnosis\") +\n",
    "  scale_color_manual(labels = c(\"Malignant\", \"Benign\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\")) +\n",
    "  theme(text = element_text(size = 12))\n",
    "perim_concav\n",
    "```\n",
    "\n",
    "In Figure \\@ref(fig:05-scatter), we can see that malignant observations typically fall in\n",
    "the upper right-hand corner of the plot area. By contrast, benign\n",
    "observations typically fall in the lower left-hand corner of the plot. In other words,\n",
    "benign observations tend to have lower concavity and perimeter values, and malignant\n",
    "ones tend to have larger values. Suppose we\n",
    "obtain a new observation not in the current data set that has all the variables\n",
    "measured *except* the label (i.e., an image without the physician's diagnosis\n",
    "for the tumor class). We could compute the standardized perimeter and concavity values,\n",
    "resulting in values of, say, 1 and 1. Could we use this information to classify\n",
    "that observation as benign or malignant? Based on the scatter plot, how might \n",
    "you classify that new observation? If the standardized concavity and perimeter\n",
    "values are 1 and 1 respectively, the point would lie in the middle of the\n",
    "orange cloud of malignant points and thus we could probably classify it as\n",
    "malignant. Based on our visualization, it seems like \n",
    "the *prediction of an unobserved label* might be possible.\n",
    "\n",
    "## Classification with $K$-nearest neighbors\n",
    "\n",
    "```{r 05-knn-0, echo = FALSE}\n",
    "## Find the distance between new point and all others in data set\n",
    "euclidDist <- function(point1, point2) {\n",
    "  # Returns the Euclidean distance between point1 and point2.\n",
    "  # Each argument is an array containing the coordinates of a point.\"\"\"\n",
    "  (sqrt(sum((point1 - point2)^2)))\n",
    "}\n",
    "distance_from_point <- function(row) {\n",
    "  euclidDist(new_point, row)\n",
    "}\n",
    "all_distances <- function(training, new_point) {\n",
    "  # Returns an array of distances\n",
    "  # between each point in the training set\n",
    "  # and the new point (which is a row of attributes)\n",
    "  distance_from_point <- function(row) {\n",
    "    euclidDist(new_point, row)\n",
    "  }\n",
    "  apply(training, MARGIN = 1, distance_from_point)\n",
    "}\n",
    "table_with_distances <- function(training, new_point) {\n",
    "  # Augments the training table\n",
    "  # with a column of distances from new_point\n",
    "  data.frame(training, Distance = all_distances(training, new_point))\n",
    "}\n",
    "new_point <- c(2, 4)\n",
    "attrs <- c(\"Perimeter\", \"Concavity\")\n",
    "my_distances <- table_with_distances(cancer[, attrs], new_point)\n",
    "neighbors <- cancer[order(my_distances$Distance), ]\n",
    "```\n",
    "\n",
    "In order to actually make predictions for new observations in practice, we\n",
    "will need a classification algorithm. \n",
    "In this book, we will use the $K$-nearest neighbors \\index{K-nearest neighbors!classification} classification algorithm.\n",
    "To predict the label of a new observation (here, classify it as either benign\n",
    "or malignant), the $K$-nearest neighbors classifier generally finds the $K$\n",
    "\"nearest\" or \"most similar\" observations in our training set, and then uses\n",
    "their diagnoses to make a prediction for the new observation's diagnosis. $K$ \n",
    "is a number that we must choose in advance; for now, we will assume that someone has chosen\n",
    "$K$ for us. We will cover how to choose $K$ ourselves in the next chapter. \n",
    "\n",
    "To illustrate the concept of $K$-nearest neighbors classification, we \n",
    "will walk through an example.  Suppose we have a\n",
    "new observation, with standardized perimeter of `r new_point[1]` and standardized concavity of `r new_point[2]`, whose \n",
    "diagnosis \"Class\" is unknown. This new observation is depicted by the red, diamond point in\n",
    "Figure \\@ref(fig:05-knn-1).\n",
    "\n",
    "```{r 05-knn-1, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap=\"Scatter plot of concavity versus perimeter with new observation represented as a red diamond.\"}\n",
    "perim_concav_with_new_point <-  bind_rows(cancer, \n",
    "                                          tibble(Perimeter = new_point[1], \n",
    "                                                 Concavity = new_point[2], \n",
    "                                                 Class = \"unknown\")) |>\n",
    "  ggplot(aes(x = Perimeter, \n",
    "             y = Concavity, \n",
    "             color = Class, \n",
    "             shape = Class, \n",
    "             size = Class)) +\n",
    "  geom_point(alpha = 0.6) +\n",
    "  labs(color = \"Diagnosis\", x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\") +\n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18))+ \n",
    "  scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(2, 2, 2.5))\n",
    "perim_concav_with_new_point\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:05-knn-2) shows that the nearest point to this new observation is **malignant** and\n",
    "located at the coordinates (`r round(neighbors[1, c(attrs[1], attrs[2])],\n",
    "1)`). The idea here is that if a point is close to another in the scatter plot,\n",
    "then the perimeter and concavity values are similar, and so we may expect that\n",
    "they would have the same diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc77c36",
   "metadata": {},
   "source": [
    "```{r 05-knn-2, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap=\"Scatter plot of concavity versus perimeter. The new observation is represented as a red diamond with a line to the one nearest neighbor, which has a malignant label.\"}\n",
    "perim_concav_with_new_point +\n",
    "  geom_segment(aes(\n",
    "    x = new_point[1],\n",
    "    y = new_point[2],\n",
    "    xend = pull(neighbors[1, attrs[1]]),\n",
    "    yend = pull(neighbors[1, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5)\n",
    "```\n",
    "\n",
    "```{r 05-knn-3, echo = FALSE}\n",
    "new_point <- c(0.2, 3.3)\n",
    "attrs <- c(\"Perimeter\", \"Concavity\")\n",
    "my_distances <- table_with_distances(cancer[, attrs], new_point)\n",
    "neighbors <- cancer[order(my_distances$Distance), ]\n",
    "```\n",
    "\n",
    "Suppose we have another new observation with standardized perimeter `r new_point[1]` and\n",
    "concavity of `r new_point[2]`. Looking at the scatter plot in Figure \\@ref(fig:05-knn-4), how would you\n",
    "classify this red, diamond observation? The nearest neighbor to this new point is a\n",
    "**benign** observation at (`r round(neighbors[1, c(attrs[1], attrs[2])], 1)`).\n",
    "Does this seem like the right prediction to make for this observation? Probably \n",
    "not, if you consider the other nearby points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afd59a",
   "metadata": {},
   "source": [
    "```{r 05-knn-4, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap=\"Scatter plot of concavity versus perimeter. The new observation is represented as a red diamond with a line to the one nearest neighbor, which has a benign label.\"}\n",
    "\n",
    "perim_concav_with_new_point2 <- bind_rows(cancer, \n",
    "                                          tibble(Perimeter = new_point[1], \n",
    "                                                 Concavity = new_point[2], \n",
    "                                                 Class = \"unknown\")) |>\n",
    "  ggplot(aes(x = Perimeter, \n",
    "             y = Concavity, \n",
    "             color = Class, \n",
    "             shape = Class, size = Class)) +\n",
    "  geom_point(alpha = 0.6) +\n",
    "  labs(color = \"Diagnosis\", \n",
    "       x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\") +\n",
    " scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18))+ \n",
    "  scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(2, 2, 2.5))\n",
    "perim_concav_with_new_point2 +  \n",
    "  geom_segment(aes(\n",
    "    x = new_point[1],\n",
    "    y = new_point[2],\n",
    "    xend = pull(neighbors[1, attrs[1]]),\n",
    "    yend = pull(neighbors[1, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5)\n",
    "```\n",
    "\n",
    "To improve the prediction we can consider several\n",
    "neighboring points, say $K = 3$, that are closest to the new observation\n",
    "to predict its diagnosis class. Among those 3 closest points, we use the\n",
    "*majority class* as our prediction for the new observation. As shown in Figure \\@ref(fig:05-knn-5), we\n",
    "see that the diagnoses of 2 of the 3 nearest neighbors to our new observation\n",
    "are malignant. Therefore we take majority vote and classify our new red, diamond\n",
    "observation as malignant. \n",
    "\n",
    "```{r 05-knn-5, echo =  FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap=\"Scatter plot of concavity versus perimeter with three nearest neighbors.\"}\n",
    "perim_concav_with_new_point2 + \n",
    "  geom_segment(aes(\n",
    "    x = new_point[1], y = new_point[2],\n",
    "    xend = pull(neighbors[1, attrs[1]]),\n",
    "    yend = pull(neighbors[1, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = new_point[1], y = new_point[2],\n",
    "    xend = pull(neighbors[2, attrs[1]]),\n",
    "    yend = pull(neighbors[2, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = new_point[1], y = new_point[2],\n",
    "    xend = pull(neighbors[3, attrs[1]]),\n",
    "    yend = pull(neighbors[3, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5)\n",
    "```\n",
    "\n",
    "Here we chose the $K=3$ nearest observations, but there is nothing special\n",
    "about $K=3$. We could have used $K=4, 5$ or more (though we may want to choose\n",
    "an odd number to avoid ties). We will discuss more about choosing $K$ in the\n",
    "next chapter. \n",
    "\n",
    "### Distance between points\n",
    "\n",
    "We decide which points are the $K$ \"nearest\" to our new observation\n",
    "using the *straight-line distance* (we will often just refer to this as *distance*). \\index{distance!K-nearest neighbors}\\index{straight line!distance}\n",
    "Suppose we have two observations $a$ and $b$, each having two predictor variables, $x$ and $y$.\n",
    "Denote $a_x$ and $a_y$ to be the values of variables $x$ and $y$ for observation $a$;\n",
    "$b_x$ and $b_y$ have similar definitions for observation $b$.\n",
    "Then the straight-line distance between observation $a$ and $b$ on the x-y plane can \n",
    "be computed using the following formula: \n",
    "\n",
    "$$\\mathrm{Distance} = \\sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}$$\n",
    "```{r 05-multiknn-0, echo = FALSE}\n",
    "new_point <- c(0, 3.5)\n",
    "```\n",
    "\n",
    "To find the $K$ nearest neighbors to our new observation, we compute the distance\n",
    "from that new observation to each observation in our training data, and select the $K$ observations corresponding to the\n",
    "$K$ *smallest* distance values. For example, suppose we want to use $K=5$ neighbors to classify a new \n",
    "observation with perimeter of `r new_point[1]` and \n",
    "concavity of `r new_point[2]`, shown as a red diamond in Figure \\@ref(fig:05-multiknn-1). Let's calculate the distances\n",
    "between our new point and each of the observations in the training set to find\n",
    "the $K=5$ neighbors that are nearest to our new point. \n",
    "You will see in the `mutate` \\index{mutate} step below, we compute the straight-line\n",
    "distance using the formula above: we square the differences between the two observations' perimeter \n",
    "and concavity coordinates, add the squared differences, and then take the square root.\n",
    "\n",
    "```{r 05-multiknn-1, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.pos = \"H\", out.extra=\"\", fig.cap=\"Scatter plot of concavity versus perimeter with new observation represented as a red diamond.\"}\n",
    "perim_concav <- bind_rows(cancer, \n",
    "                          tibble(Perimeter = new_point[1], \n",
    "                                 Concavity = new_point[2], \n",
    "                                 Class = \"unknown\")) |>\n",
    "  ggplot(aes(x = Perimeter, \n",
    "             y = Concavity, \n",
    "             color = Class, \n",
    "             shape = Class, \n",
    "             size = Class)) +\n",
    "  geom_point(aes(x = new_point[1], \n",
    "                 y = new_point[2]), \n",
    "             color = \"red\", \n",
    "             size = 2.5, \n",
    "             pch = 18) + \n",
    "  geom_point(alpha = 0.5) +\n",
    "  scale_x_continuous(name = \"Perimeter (standardized)\", \n",
    "                     breaks = seq(-2, 4, 1)) +\n",
    "  scale_y_continuous(name = \"Concavity (standardized)\", \n",
    "                     breaks = seq(-2, 4, 1)) +\n",
    "  labs(color = \"Diagnosis\") + \n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18))+ \n",
    "  scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(2, 2, 2.5))\n",
    "\n",
    "perim_concav\n",
    "```\n",
    "\n",
    "```{r 05-multiknn-2}\n",
    "new_obs_Perimeter <- 0\n",
    "new_obs_Concavity <- 3.5\n",
    "cancer |>\n",
    "  select(ID, Perimeter, Concavity, Class) |>\n",
    "  mutate(dist_from_new = sqrt((Perimeter - new_obs_Perimeter)^2 + \n",
    "                              (Concavity - new_obs_Concavity)^2)) |>\n",
    "  arrange(dist_from_new) |>\n",
    "  slice(1:5) # take the first 5 rows\n",
    "```\n",
    "\n",
    "In Table \\@ref(tab:05-multiknn-mathtable) we show in mathematical detail how\n",
    "the `mutate` step was used to compute the `dist_from_new` variable (the\n",
    "distance to the new observation) for each of the 5 nearest neighbors in the\n",
    "training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff845aa",
   "metadata": {},
   "source": [
    "```{r 05-multiknn-4, echo = FALSE}\n",
    "my_distances <- table_with_distances(cancer[, attrs], new_point)\n",
    "neighbors <- my_distances[order(my_distances$Distance), ]\n",
    "k <- 5\n",
    "tab <- data.frame(neighbors[1:k, ], \n",
    "                  cancer[order(my_distances$Distance), ][1:k, c(\"ID\", \"Class\")])\n",
    "\n",
    "\n",
    "math_table <- tibble(Perimeter = round(tab[1:5,1],2), \n",
    "                     Concavity = round(tab[1:5,2],2), \n",
    "                          dist = round(neighbors[1:5, \"Distance\"], 2)\n",
    "                    )\n",
    "math_table <- math_table |> \n",
    "                    mutate(Distance = paste0(\"$\\\\sqrt{(\", new_obs_Perimeter, \" - \", ifelse(Perimeter < 0, \"(\", \"\"), Perimeter, ifelse(Perimeter < 0,\")\",\"\"), \")^2\",\n",
    "                                             \" + \",\n",
    "                                             \"(\", new_obs_Concavity, \" - \", ifelse(Concavity < 0,\"(\",\"\"), Concavity, ifelse(Concavity < 0,\")\",\"\"), \")^2} = \", dist, \"$\")) |>\n",
    "                    select(-dist) |>\n",
    "                    mutate(Class= tab[1:5, \"Class\"])\n",
    "```\n",
    "\n",
    "```{r 05-multiknn-mathtable, echo = FALSE}\n",
    "kable(math_table, booktabs = TRUE, \n",
    "      caption = \"Evaluating the distances from the new observation to each of its 5 nearest neighbors\", \n",
    "      escape = FALSE) |>\n",
    "  kable_styling(latex_options = \"hold_position\")\n",
    "```\n",
    "\n",
    "The result of this computation shows that 3 of the 5 nearest neighbors to our new observation are\n",
    "malignant (`M`); since this is the majority, we classify our new observation as malignant. \n",
    "These 5 neighbors are circled in Figure \\@ref(fig:05-multiknn-3).\n",
    "\n",
    "```{r 05-multiknn-3, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap=\"Scatter plot of concavity versus perimeter with 5 nearest neighbors circled.\"}\n",
    "perim_concav + annotate(\"path\",\n",
    "  x = new_point[1] + 1.4 * cos(seq(0, 2 * pi,\n",
    "    length.out = 100\n",
    "  )),\n",
    "  y = new_point[2] + 1.4 * sin(seq(0, 2 * pi,\n",
    "    length.out = 100\n",
    "  ))\n",
    ")\n",
    "```\n",
    "\n",
    "### More than two explanatory variables \n",
    "\n",
    "Although the above description is directed toward two predictor variables, \n",
    "exactly the same $K$-nearest neighbors algorithm applies when you\n",
    "have a higher number of predictor variables.  Each predictor variable may give us new\n",
    "information to help create our classifier.  The only difference is the formula\n",
    "for the distance between points. Suppose we have $m$ predictor\n",
    "variables for two observations $a$ and $b$, i.e., \n",
    "$a = (a_{1}, a_{2}, \\dots, a_{m})$ and\n",
    "$b = (b_{1}, b_{2}, \\dots, b_{m})$.\n",
    "\n",
    "The distance formula becomes \\index{distance!more than two variables}\n",
    "\n",
    "$$\\mathrm{Distance} = \\sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \\dots + (a_{m} - b_{m})^2}.$$\n",
    "\n",
    "This formula still corresponds to a straight-line distance, just in a space\n",
    "with more dimensions. Suppose we want to calculate the distance between a new\n",
    "observation with a perimeter of 0, concavity of 3.5, and symmetry of 1, and\n",
    "another observation with a perimeter, concavity, and symmetry of 0.417, 2.31, and\n",
    "0.837 respectively. We have two observations with three predictor variables:\n",
    "perimeter, concavity, and symmetry. Previously, when we had two variables, we\n",
    "added up the squared difference between each of our (two) variables, and then\n",
    "took the square root. Now we will do the same, except for our three variables.\n",
    "We calculate the distance as follows\n",
    "\n",
    "$$\\mathrm{Distance} =\\sqrt{(0 - 0.417)^2 + (3.5 - 2.31)^2 + (1 - 0.837)^2} = 1.27.$$\n",
    "\n",
    "Let's calculate the distances between our new observation and each of the\n",
    "observations in the training set to find the $K=5$ neighbors when we have these\n",
    "three predictors. \n",
    "```{r}\n",
    "new_obs_Perimeter <- 0\n",
    "new_obs_Concavity <- 3.5\n",
    "new_obs_Symmetry <- 1\n",
    "\n",
    "cancer |>\n",
    "  select(ID, Perimeter, Concavity, Symmetry, Class) |>\n",
    "  mutate(dist_from_new = sqrt((Perimeter - new_obs_Perimeter)^2 + \n",
    "                              (Concavity - new_obs_Concavity)^2 +\n",
    "                                (Symmetry - new_obs_Symmetry)^2)) |>\n",
    "  arrange(dist_from_new) |>\n",
    "  slice(1:5) # take the first 5 rows\n",
    "```\n",
    "\n",
    "Based on $K=5$ nearest neighbors with these three predictors we would classify the new observation as malignant since 4 out of 5 of the nearest neighbors are malignant class. \n",
    "Figure \\@ref(fig:05-more) shows what the data look like when we visualize them \n",
    "as a 3-dimensional scatter with lines from the new observation to its five nearest neighbors.\n",
    "\n",
    "```{r 05-more, echo = FALSE, message = FALSE, fig.cap = \"3D scatter plot of the standardized symmetry, concavity, and perimeter variables. Note that in general we recommend against using 3D visualizations; here we show the data in 3D only to illustrate what higher dimensions and nearest neighbors look like, for learning purposes.\", fig.retina=2, out.width=\"100%\"}\n",
    "attrs <- c(\"Perimeter\", \"Concavity\", \"Symmetry\")\n",
    "\n",
    "# create new scaled obs and get NNs\n",
    "new_obs_3 <- tibble(Perimeter = 0, \n",
    "                    Concavity = 3.5, \n",
    "                    Symmetry = 1, \n",
    "                    Class = \"Unknown\")\n",
    "my_distances_3 <- table_with_distances(cancer[, attrs], \n",
    "                                       new_obs_3[, attrs])\n",
    "neighbors_3 <- cancer[order(my_distances_3$Distance), ]\n",
    "\n",
    "data <- neighbors_3 |> select(Perimeter, Concavity, Symmetry) |> slice(1:5)\n",
    "\n",
    "# add to the df\n",
    "scaled_cancer_3 <- bind_rows(cancer, new_obs_3) |> \n",
    "  mutate(Class = fct_recode(Class, \"Benign\" = \"B\", \"Malignant\"= \"M\"))\n",
    "\n",
    "plot_3d <- scaled_cancer_3 |>\n",
    "  plot_ly() |>\n",
    "  layout(scene = list(\n",
    "    xaxis = list(title = \"Perimeter\", titlefont = list(size = 14)),\n",
    "    yaxis = list(title = \"Concavity\", titlefont = list(size = 14)),\n",
    "    zaxis = list(title = \"Symmetry\", titlefont = list(size = 14))\n",
    "  )) |> \n",
    "  add_trace(x = ~Perimeter,\n",
    "            y = ~Concavity,\n",
    "            z = ~Symmetry,\n",
    "            color = ~Class,\n",
    "            opacity = 0.4,\n",
    "            size = 2,\n",
    "            colors = c(\"orange2\", \"steelblue2\", \"red\"), \n",
    "            symbol = ~Class, symbols = c('circle','circle','diamond'))\n",
    "\n",
    "x1 <- c(pull(new_obs_3[1]), data$Perimeter[1])\n",
    "y1 <- c(pull(new_obs_3[2]), data$Concavity[1])\n",
    "z1 <- c(pull(new_obs_3[3]), data$Symmetry[1])\n",
    "\n",
    "x2 <- c(pull(new_obs_3[1]), data$Perimeter[2])\n",
    "y2 <- c(pull(new_obs_3[2]), data$Concavity[2])\n",
    "z2 <- c(pull(new_obs_3[3]), data$Symmetry[2])\n",
    "\n",
    "x3 <- c(pull(new_obs_3[1]), data$Perimeter[3])\n",
    "y3 <- c(pull(new_obs_3[2]), data$Concavity[3])\n",
    "z3 <- c(pull(new_obs_3[3]), data$Symmetry[3])\n",
    "\n",
    "x4 <- c(pull(new_obs_3[1]), data$Perimeter[4])\n",
    "y4 <- c(pull(new_obs_3[2]), data$Concavity[4])\n",
    "z4 <- c(pull(new_obs_3[3]), data$Symmetry[4])\n",
    "\n",
    "x5 <- c(pull(new_obs_3[1]), data$Perimeter[5])\n",
    "y5 <- c(pull(new_obs_3[2]), data$Concavity[5])\n",
    "z5 <- c(pull(new_obs_3[3]), data$Symmetry[5])\n",
    "\n",
    "plot_3d <- plot_3d  |>\n",
    "  add_trace(x = x1, y = y1, z = z1, type = \"scatter3d\", mode = \"lines\", \n",
    "            name = \"lines\", showlegend = FALSE, color = I(\"steelblue2\")) |>\n",
    "  add_trace(x = x2, y = y2, z = z2, type = \"scatter3d\", mode = \"lines\", \n",
    "            name = \"lines\", showlegend = FALSE, color =  I(\"steelblue2\")) |>\n",
    "  add_trace(x = x3, y = y3, z = z3, type = \"scatter3d\", mode = \"lines\", \n",
    "            name = \"lines\", showlegend = FALSE, color =  I(\"steelblue2\")) |>\n",
    "  add_trace(x = x4, y = y4, z = z4, type = \"scatter3d\", mode = \"lines\", \n",
    "            name = \"lines\", showlegend = FALSE, color =  I(\"orange2\")) |>\n",
    "  add_trace(x = x5, y = y5, z = z5, type = \"scatter3d\", mode = \"lines\", \n",
    "            name = \"lines\", showlegend = FALSE, color =  I(\"steelblue2\"))\n",
    "\n",
    "if(!is_latex_output()){  \n",
    "  plot_3d\n",
    "} else {\n",
    "  # scene = list(camera = list(eye = list(x=2, y=2, z = 1.5)))\n",
    "  # plot_3d <- plot_3d  |> layout(scene = scene)\n",
    "  # save_image(plot_3d, \"img/plot3d_knn_classification.png\", scale = 10)\n",
    "  # cannot adjust size of points in this plot for pdf \n",
    "  # so using a screenshot for now instead\n",
    "  knitr::include_graphics(\"img/plot3d_knn_classification.png\")\n",
    "}\n",
    "```\n",
    "\n",
    "### Summary of $K$-nearest neighbors algorithm\n",
    "\n",
    "In order to classify a new observation using a $K$-nearest neighbor classifier, we have to do the following:\n",
    "\n",
    "1. Compute the distance between the new observation and each observation in the training set.\n",
    "2. Sort the data table in ascending order according to the distances.\n",
    "3. Choose the top $K$ rows of the sorted table.\n",
    "4. Classify the new observation based on a majority vote of the neighbor classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daffc8d",
   "metadata": {},
   "source": [
    "## $K$-nearest neighbors with `tidymodels`\n",
    "\n",
    "Coding the $K$-nearest neighbors algorithm in R ourselves can get complicated,\n",
    "especially if we want to handle multiple classes, more than two variables,\n",
    "or predict the class for multiple new observations. Thankfully, in R,\n",
    "the $K$-nearest neighbors algorithm is \n",
    "implemented in [the `parsnip` R package](https://parsnip.tidymodels.org/) [@parsnip] \n",
    "included in `tidymodels`, along with \n",
    "many [other models](https://www.tidymodels.org/find/parsnip/) \\index{tidymodels}\\index{parsnip}\n",
    " that you will encounter in this and future chapters of the book. The `tidymodels` collection\n",
    "provides tools to help make and use models, such as classifiers.  Using the packages\n",
    "in this collection will help keep our code simple, readable and accurate; the \n",
    "less we have to code ourselves, the fewer mistakes we will likely make. We \n",
    "start by loading `tidymodels`.\n",
    "\n",
    "```{r 05-tidymodels, warning = FALSE, message = FALSE}\n",
    "library(tidymodels)\n",
    "```\n",
    "\n",
    "Let's walk through how to use `tidymodels` to perform $K$-nearest neighbors classification. \n",
    "We will use the `cancer` data set from above, with\n",
    "perimeter and concavity as predictors and $K = 5$ neighbors to build our classifier. Then\n",
    "we will use the classifier to predict the diagnosis label for a new observation with\n",
    "perimeter 0, concavity 3.5, and an unknown diagnosis label. Let's pick out our two desired\n",
    "predictor variables and class label and store them as a new data set named `cancer_train`:\n",
    "\n",
    "```{r 05-tidymodels-2}\n",
    "cancer_train <- cancer |>\n",
    "  select(Class, Perimeter, Concavity)\n",
    "cancer_train\n",
    "```\n",
    "\n",
    "Next, we create a *model specification* for \\index{tidymodels!model specification} $K$-nearest neighbors classification\n",
    "by calling the `nearest_neighbor` function, specifying that we want to use $K = 5$ neighbors\n",
    "(we will discuss how to choose $K$ in the next chapter) and the straight-line \n",
    "distance (`weight_func = \"rectangular\"`). The `weight_func` argument controls\n",
    "how neighbors vote when classifying a new observation; by setting it to `\"rectangular\"`,\n",
    "each of the $K$ nearest neighbors gets exactly 1 vote as described above. Other choices, \n",
    "which weigh each neighbor's vote differently, can be found on \n",
    "[the `parsnip` website](https://parsnip.tidymodels.org/reference/nearest_neighbor.html).\n",
    "In the `set_engine` \\index{tidymodels!engine} argument, we specify which package or system will be used for training\n",
    "the model. Here `kknn` is the R package we will use for performing $K$-nearest neighbors classification.\n",
    "Finally, we specify that this is a classification problem with the `set_mode` function.\n",
    "\n",
    "```{r 05-tidymodels-3}\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "knn_spec\n",
    "```\n",
    "\n",
    "In order to fit the model on the breast cancer data, we need to pass the model specification\n",
    "and the data set to the `fit` function. We also need to specify what variables to use as predictors\n",
    "and what variable to use as the target. Below, the `Class ~ Perimeter + Concavity` argument specifies \n",
    "that `Class` is the target variable (the one we want to predict),\n",
    "and both `Perimeter` and `Concavity` are to be used as the predictors.\n",
    "\n",
    "```{r 05-tidymodels-4}\n",
    "knn_fit <- knn_spec |>\n",
    "  fit(Class ~ Perimeter + Concavity, data = cancer_train)\n",
    "```\n",
    "\n",
    "We can also use a convenient shorthand syntax using a period, `Class ~ .`, to indicate\n",
    "that we want to use every variable *except* `Class` \\index{tidymodels!model formula} as a predictor in the model.\n",
    "In this particular setup, since `Concavity` and `Perimeter` are the only two predictors in the `cancer_train`\n",
    "data frame, `Class ~ Perimeter + Concavity` and `Class ~ .` are equivalent.\n",
    "In general, you can choose individual predictors using the `+` symbol, or you can specify to\n",
    "use *all* predictors using the `.` symbol.\n",
    "\n",
    "```{r 05-tidymodels-4b, results = 'hide', echo = TRUE}\n",
    "knn_fit <- knn_spec |>\n",
    "  fit(Class ~ ., data = cancer_train)\n",
    "knn_fit\n",
    "```\n",
    "\n",
    "```{r echo = FALSE}\n",
    "print_tidymodels(knn_fit)\n",
    "```\n",
    "\n",
    "Here you can see the final trained model summary. It confirms that the computational engine used\n",
    "to train the model  was `kknn::train.kknn`. It also shows the fraction of errors made by\n",
    "the nearest neighbor model, but we will ignore this for now and discuss it in more detail\n",
    "in the next chapter.\n",
    "Finally, it shows (somewhat confusingly) that the \"best\" weight function \n",
    "was \"rectangular\" and \"best\" setting of $K$ was 5; but since we specified these earlier,\n",
    "R is just repeating those settings to us here. In the next chapter, we will actually\n",
    "let R find the value of $K$ for us. \n",
    "\n",
    "Finally, we make the prediction on the new observation by calling the `predict` \\index{tidymodels!predict} function,\n",
    "passing both the fit object we just created and the new observation itself. As above, \n",
    "when we ran the $K$-nearest neighbors\n",
    "classification algorithm manually, the `knn_fit` object classifies the new observation as \n",
    "malignant (\"M\"). Note that the `predict` function outputs a data frame with a single \n",
    "variable named `.pred_class`.\n",
    "\n",
    "```{r 05-predict}\n",
    "new_obs <- tibble(Perimeter = 0, Concavity = 3.5)\n",
    "predict(knn_fit, new_obs)\n",
    "```\n",
    "\n",
    "Is this predicted malignant label the true class for this observation? \n",
    "Well, we don't know because we do not have this\n",
    "observation's diagnosis&mdash; that is what we were trying to predict! The \n",
    "classifier's prediction is not necessarily correct, but in the next chapter, we will \n",
    "learn ways to quantify how accurate we think our predictions are.\n",
    "\n",
    "## Data preprocessing with `tidymodels`\n",
    "\n",
    "### Centering and scaling\n",
    "\n",
    "When using $K$-nearest neighbor classification, the *scale* \\index{scaling} of each variable\n",
    "(i.e., its size and range of values) matters. Since the classifier predicts\n",
    "classes by identifying observations nearest to it, any variables with \n",
    "a large scale will have a much larger effect than variables with a small\n",
    "scale. But just because a variable has a large scale *doesn't mean* that it is\n",
    "more important for making accurate predictions. For example, suppose you have a\n",
    "data set with two features, salary (in dollars) and years of education, and\n",
    "you want to predict the corresponding type of job. When we compute the\n",
    "neighbor distances, a difference of \\$1000 is huge compared to a difference of\n",
    "10 years of education. But for our conceptual understanding and answering of\n",
    "the problem, it's the opposite; 10 years of education is huge compared to a\n",
    "difference of \\$1000 in yearly salary!\n",
    "\n",
    "In many other predictive models, the *center* of each variable (e.g., its mean)\n",
    "matters as well. For example, if we had a data set with a temperature variable\n",
    "measured in degrees Kelvin, and the same data set with temperature measured in\n",
    "degrees Celsius, the two variables would differ by a constant shift of 273\n",
    "(even though they contain exactly the same information). Likewise, in our\n",
    "hypothetical job classification example, we would likely see that the center of\n",
    "the salary variable is in the tens of thousands, while the center of the years\n",
    "of education variable is in the single digits. Although this doesn't affect the\n",
    "$K$-nearest neighbor classification algorithm, this large shift can change the\n",
    "outcome of using many other predictive models.  \\index{centering}\n",
    "\n",
    "To scale and center our data, we need to find\n",
    "our variables' *mean* (the average, which quantifies the \"central\" value of a \n",
    "set of numbers) and *standard deviation* (a number quantifying how spread out values are). \n",
    "For each observed value of the variable, we subtract the mean (i.e., center the variable) \n",
    "and divide by the standard deviation (i.e., scale the variable). When we do this, the data \n",
    "is said to be *standardized*, \\index{standardization!K-nearest neighbors} and all variables in a data set will have a mean of 0 \n",
    "and a standard deviation of 1. To illustrate the effect that standardization can have on the $K$-nearest\n",
    "neighbor algorithm, we will read in the original, unstandardized Wisconsin breast\n",
    "cancer data set; we have been using a standardized version of the data set up\n",
    "until now. To keep things simple, we will just use the `Area`, `Smoothness`, and `Class`\n",
    "variables:\n",
    "\n",
    "```{r 05-scaling-1, message = FALSE}\n",
    "unscaled_cancer <- read_csv(\"data/unscaled_wdbc.csv\") |>\n",
    "  mutate(Class = as_factor(Class)) |>\n",
    "  select(Class, Area, Smoothness)\n",
    "unscaled_cancer\n",
    "```\n",
    "\n",
    "Looking at the unscaled and uncentered data above, you can see that the differences\n",
    "between the values for area measurements are much larger than those for\n",
    "smoothness. Will this affect\n",
    "predictions? In order to find out, we will create a scatter plot of these two\n",
    "predictors (colored by diagnosis) for both the unstandardized data we just\n",
    "loaded, and the standardized version of that same data. But first, we need to\n",
    "standardize the `unscaled_cancer` data set with `tidymodels`.\n",
    "\n",
    "In the `tidymodels` framework, all data preprocessing happens \n",
    "using a `recipe` from [the `recipes` R package](https://recipes.tidymodels.org/) [@recipes]\n",
    "Here we will initialize a recipe \\index{recipe} \\index{tidymodels!recipe|see{recipe}} for \n",
    "the `unscaled_cancer` data above, specifying\n",
    "that the `Class` variable is the target, and all other variables are predictors:\n",
    "\n",
    "```{r 05-scaling-2}\n",
    "uc_recipe <- recipe(Class ~ ., data = unscaled_cancer)\n",
    "print(uc_recipe)\n",
    "```\n",
    "\n",
    "So far, there is not much in the recipe; just a statement about the number of targets\n",
    "and predictors. Let's add \n",
    "scaling (`step_scale`) \\index{recipe!step\\_scale} and \n",
    "centering (`step_center`) \\index{recipe!step\\_center} steps for \n",
    "all of the predictors so that they each have a mean of 0 and standard deviation of 1.\n",
    "Note that `tidyverse` actually provides `step_normalize`, which does both centering and scaling in\n",
    "a single recipe step; in this book we will keep `step_scale` and `step_center` separate\n",
    "to emphasize conceptually that there are two steps happening. \n",
    "The `prep` function finalizes the recipe by using the data (here, `unscaled_cancer`)  \\index{tidymodels!prep}\\index{prep|see{tidymodels}}\n",
    "to compute anything necessary to run the recipe (in this case, the column means and standard\n",
    "deviations):\n",
    "\n",
    "```{r 05-scaling-3}\n",
    "uc_recipe <- uc_recipe |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors()) |>\n",
    "  prep()\n",
    "uc_recipe\n",
    "```\n",
    "\n",
    "You can now see that the recipe includes a scaling and centering step for all predictor variables.\n",
    "Note that when you add a step to a recipe, you must specify what columns to apply the step to.\n",
    "Here we used the `all_predictors()` \\index{recipe!all\\_predictors} function to specify that each step should be applied to \n",
    "all predictor variables. However, there are a number of different arguments one could use here,\n",
    "as well as naming particular columns with the same syntax as the `select` function. \n",
    "For example:\n",
    "\n",
    "- `all_nominal()` and `all_numeric()`: specify all categorical or all numeric variables\n",
    "- `all_predictors()` and `all_outcomes()`: specify all predictor or all target variables\n",
    "- `Area, Smoothness`: specify both the `Area` and `Smoothness` variable\n",
    "- `-Class`: specify everything except the `Class` variable\n",
    "\n",
    "You can find a full set of all the steps and variable selection functions\n",
    "on the [`recipes` reference page](https://recipes.tidymodels.org/reference/index.html).\n",
    "\n",
    "At this point, we have calculated the required statistics based on the data input into the \n",
    "recipe, but the data are not yet scaled and centered. To actually scale and center \n",
    "the data, we need to apply the `bake` \\index{tidymodels!bake} \\index{bake|see{tidymodels}} function to the unscaled data.\n",
    "\n",
    "```{r 05-scaling-4}\n",
    "scaled_cancer <- bake(uc_recipe, unscaled_cancer)\n",
    "scaled_cancer\n",
    "```\n",
    "\n",
    "It may seem redundant that we had to both `bake` *and* `prep` to scale and center the data.\n",
    " However, we do this in two steps so we can specify a different data set in the `bake` step if we want. \n",
    " For example, we may want to specify new data that were not part of the training set. \n",
    "\n",
    "You may wonder why we are doing so much work just to center and\n",
    "scale our variables. Can't we just manually scale and center the `Area` and\n",
    "`Smoothness` variables ourselves before building our $K$-nearest neighbor model? Well,\n",
    "technically *yes*; but doing so is error-prone.  In particular, we might\n",
    "accidentally forget to apply the same centering / scaling when making\n",
    "predictions, or accidentally apply a *different* centering / scaling than what\n",
    "we used while training. Proper use of a `recipe` helps keep our code simple,\n",
    "readable, and error-free. Furthermore, note that using `prep` and `bake` is\n",
    "required only when you want to inspect the result of the preprocessing steps\n",
    "yourself. You will see further on in Section\n",
    "\\@ref(puttingittogetherworkflow) that `tidymodels` provides tools to\n",
    "automatically apply `prep` and `bake` as necessary without additional coding effort.\n",
    "\n",
    "Figure \\@ref(fig:05-scaling-plt) shows the two scatter plots side-by-side&mdash;one for `unscaled_cancer` and one for\n",
    "`scaled_cancer`. Each has the same new observation annotated with its $K=3$ nearest neighbors.\n",
    "In the original unstandardized data plot, you can see some odd choices\n",
    "for the three nearest neighbors. In particular, the \"neighbors\" are visually\n",
    "well within the cloud of benign observations, and the neighbors are all nearly\n",
    "vertically aligned with the new observation (which is why it looks like there\n",
    "is only one black line on this plot). Figure \\@ref(fig:05-scaling-plt-zoomed)\n",
    "shows a close-up of that region on the unstandardized plot. Here the computation of nearest\n",
    "neighbors is dominated by the much larger-scale area variable. The plot for standardized data \n",
    "on the right in Figure \\@ref(fig:05-scaling-plt) shows a much more intuitively reasonable\n",
    "selection of nearest neighbors. Thus, standardizing the data can change things\n",
    "in an important way when we are using predictive algorithms. \n",
    "Standardizing your data should be a part of the preprocessing you do\n",
    "before predictive modeling and you should always think carefully about your problem domain and\n",
    "whether you need to standardize your data. \n",
    "\n",
    "```{r 05-scaling-plt, echo = FALSE, fig.height = 4, fig.cap = \"Comparison of K = 3 nearest neighbors with standardized and unstandardized data.\"}\n",
    "\n",
    "attrs <- c(\"Area\", \"Smoothness\")\n",
    "\n",
    "# create a new obs and get its NNs\n",
    "new_obs <- tibble(Area = 400, Smoothness = 0.135, Class = \"unknown\")\n",
    "my_distances <- table_with_distances(unscaled_cancer[, attrs], \n",
    "                                     new_obs[, attrs])\n",
    "neighbors <- unscaled_cancer[order(my_distances$Distance), ]\n",
    "\n",
    "# add the new obs to the df\n",
    "unscaled_cancer <- bind_rows(unscaled_cancer, new_obs)\n",
    "\n",
    "# plot the scatter\n",
    "unscaled <- ggplot(unscaled_cancer, aes(x = Area, \n",
    "                                        y = Smoothness, \n",
    "                                        group = Class, \n",
    "                                        color = Class, \n",
    "                                        shape = Class, size = Class)) +\n",
    "  geom_point(alpha = 0.6) + \n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18)) +\n",
    "    scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values=c(2,2,2.5)) + \n",
    "  ggtitle(\"Unstandardized Data\") +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[1, attrs[1]]),\n",
    "    yend = unlist(neighbors[1, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[2, attrs[1]]),\n",
    "    yend = unlist(neighbors[2, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[3, attrs[1]]),\n",
    "    yend = unlist(neighbors[3, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5)\n",
    "\n",
    "# create new scaled obs and get NNs\n",
    "new_obs_scaled <- tibble(Area = -0.72, Smoothness = 2.8, Class = \"unknown\")\n",
    "my_distances_scaled <- table_with_distances(scaled_cancer[, attrs], \n",
    "                                            new_obs_scaled[, attrs])\n",
    "neighbors_scaled <- scaled_cancer[order(my_distances_scaled$Distance), ]\n",
    "\n",
    "# add to the df\n",
    "scaled_cancer <- bind_rows(scaled_cancer, new_obs_scaled)\n",
    "\n",
    "# plot the scatter\n",
    "scaled <- ggplot(scaled_cancer, aes(x = Area, \n",
    "                                    y = Smoothness, \n",
    "                                    group = Class, \n",
    "                                    color = Class, \n",
    "                                    shape = Class, \n",
    "                                    size = Class)) +\n",
    "  geom_point(alpha = 0.6) + \n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18)) +\n",
    "  scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                    values=c(2,2,2.5)) + \n",
    "  ggtitle(\"Standardized Data\") +\n",
    "  labs(x = \"Area (standardized)\", y = \"Smoothness (standardized)\") + \n",
    "  # coord_equal(ratio = 1) +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs_scaled[1]), y = unlist(new_obs_scaled[2]),\n",
    "    xend = unlist(neighbors_scaled[1, attrs[1]]),\n",
    "    yend = unlist(neighbors_scaled[1, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs_scaled[1]), y = unlist(new_obs_scaled[2]),\n",
    "    xend = unlist(neighbors_scaled[2, attrs[1]]),\n",
    "    yend = unlist(neighbors_scaled[2, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5) +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs_scaled[1]), y = unlist(new_obs_scaled[2]),\n",
    "    xend = unlist(neighbors_scaled[3, attrs[1]]),\n",
    "    yend = unlist(neighbors_scaled[3, attrs[2]])\n",
    "  ), color = \"black\", size = 0.5)\n",
    "\n",
    "ggarrange(unscaled, scaled, ncol = 2, common.legend = TRUE, legend = \"bottom\")\n",
    "\n",
    "```\n",
    "\n",
    "```{r 05-scaling-plt-zoomed, fig.height = 4.5, fig.width = 9, echo = FALSE, fig.cap = \"Close-up of three nearest neighbors for unstandardized data.\"}\n",
    "library(ggforce)\n",
    "ggplot(unscaled_cancer, aes(x = Area, \n",
    "                            y = Smoothness, \n",
    "                            group = Class, \n",
    "                            color = Class, \n",
    "                            shape = Class)) +\n",
    "  geom_point(size = 2.5, alpha = 0.6) + \n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                   labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18)) +\n",
    "    scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values = c(1, 1, 2.5)) + \n",
    "  ggtitle(\"Unstandardized Data\") +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[1, attrs[1]]),\n",
    "    yend = unlist(neighbors[1, attrs[2]])\n",
    "  ), color = \"black\") +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[2, attrs[1]]),\n",
    "    yend = unlist(neighbors[2, attrs[2]])\n",
    "  ), color = \"black\") +\n",
    "  geom_segment(aes(\n",
    "    x = unlist(new_obs[1]), y = unlist(new_obs[2]),\n",
    "    xend = unlist(neighbors[3, attrs[1]]),\n",
    "    yend = unlist(neighbors[3, attrs[2]])\n",
    "  ), color = \"black\") +  \n",
    "   facet_zoom(x = ( Area > 380 & Area < 420) , \n",
    "              y = (Smoothness > 0.08 & Smoothness < 0.14), zoom.size = 2) + \n",
    "    theme_bw() + \n",
    "    theme(text = element_text(size = 18), axis.title=element_text(size=18), legend.position=\"bottom\")\n",
    "```\n",
    "\n",
    "### Balancing\n",
    "\n",
    "Another potential issue in a data set for a classifier is *class imbalance*, \\index{balance}\\index{imbalance}\n",
    "i.e., when one label is much more common than another. Since classifiers like\n",
    "the $K$-nearest neighbor algorithm use the labels of nearby points to predict\n",
    "the label of a new point, if there are many more data points with one label\n",
    "overall, the algorithm is more likely to pick that label in general (even if\n",
    "the \"pattern\" of data suggests otherwise). Class imbalance is actually quite a\n",
    "common and important problem: from rare disease diagnosis to malicious email\n",
    "detection, there are many cases in which the \"important\" class to identify\n",
    "(presence of disease, malicious email) is much rarer than the \"unimportant\"\n",
    "class (no disease, normal email).\n",
    "\n",
    "To better illustrate the problem, let's revisit the scaled breast cancer data, \n",
    "`cancer`; except now we will remove many of the observations of malignant tumors, simulating\n",
    "what the data would look like if the cancer was rare. We will do this by\n",
    "picking only 3 observations from the malignant group, and keeping all\n",
    "of the benign observations. We choose these 3 observations using the `slice_head`\n",
    "function, which takes two arguments: a data frame-like object,\n",
    "and the number of rows to select from the top (`n`).\n",
    "The new imbalanced data is shown in Figure \\@ref(fig:05-unbalanced).\n",
    "\n",
    "```{r 05-unbalanced-seed, echo = FALSE, fig.height = 3.5, fig.width = 4.5, warning = FALSE, message = FALSE}\n",
    "# hidden seed here for reproducibility \n",
    "# randomness shouldn't affect much in this use of step_upsample,\n",
    "# but just in case...\n",
    "set.seed(3)\n",
    "```\n",
    "\n",
    "```{r 05-unbalanced, fig.height = 3.5, fig.width = 4.5, fig.pos = \"H\", out.extra=\"\", fig.cap = \"Imbalanced data.\"}\n",
    "rare_cancer <- bind_rows(\n",
    "      filter(cancer, Class == \"B\"),\n",
    "      cancer |> filter(Class == \"M\") |> slice_head(n = 3)\n",
    "    ) |>\n",
    "    select(Class, Perimeter, Concavity)\n",
    "\n",
    "rare_plot <- rare_cancer |>\n",
    "  ggplot(aes(x = Perimeter, y = Concavity, color = Class)) +\n",
    "  geom_point(alpha = 0.5) +\n",
    "  labs(x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\",\n",
    "       color = \"Diagnosis\") +\n",
    "  scale_color_manual(labels = c(\"Malignant\", \"Benign\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\")) +\n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "rare_plot\n",
    "```\n",
    "\n",
    "Suppose we now decided to use $K = 7$ in $K$-nearest neighbor classification.\n",
    "With only 3 observations of malignant tumors, the classifier \n",
    "will *always predict that the tumor is benign, no matter what its concavity and perimeter\n",
    "are!* This is because in a majority vote of 7 observations, at most 3 will be\n",
    "malignant (we only have 3 total malignant observations), so at least 4 must be\n",
    "benign, and the benign vote will always win. For example, Figure \\@ref(fig:05-upsample)\n",
    "shows what happens for a new tumor observation that is quite close to three observations\n",
    "in the training data that were tagged as malignant.\n",
    "\n",
    "```{r 05-upsample, echo=FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Imbalanced data with 7 nearest neighbors to a new observation highlighted.\"}\n",
    "new_point <- c(2, 2)\n",
    "attrs <- c(\"Perimeter\", \"Concavity\")\n",
    "my_distances <- table_with_distances(rare_cancer[, attrs], new_point)\n",
    "my_distances <- bind_cols(my_distances, select(rare_cancer, Class))\n",
    "neighbors <- rare_cancer[order(my_distances$Distance), ]\n",
    "\n",
    "\n",
    "rare_plot <- bind_rows(rare_cancer, \n",
    "                       tibble(Perimeter = new_point[1], \n",
    "                              Concavity = new_point[2], \n",
    "                              Class = \"unknown\")) |>\n",
    "  ggplot(aes(x = Perimeter, y = Concavity, color = Class, shape = Class)) +\n",
    "  geom_point(alpha = 0.5) +\n",
    "  labs(color = \"Diagnosis\", \n",
    "       x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\") + \n",
    "  scale_color_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"), \n",
    "                     values = c(\"steelblue2\", \"orange2\", \"red\")) +\n",
    "  scale_shape_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(16, 16, 18))+ \n",
    "  scale_size_manual(name = \"Diagnosis\", \n",
    "                     labels = c(\"Benign\", \"Malignant\", \"Unknown\"),\n",
    "                     values= c(2, 2, 2.5))\n",
    "\n",
    "for (i in 1:7) {\n",
    "  clr <- \"steelblue2\"\n",
    "  if (neighbors$Class[i] == \"M\") {\n",
    "    clr <- \"orange2\"\n",
    "  }\n",
    "  rare_plot <- rare_plot +\n",
    "    geom_segment(\n",
    "      x = new_point[1],\n",
    "      y = new_point[2],\n",
    "      xend = pull(neighbors[i, attrs[1]]),\n",
    "      yend = pull(neighbors[i, attrs[2]]), color = clr\n",
    "    )\n",
    "}\n",
    "rare_plot + geom_point(aes(x = new_point[1], y = new_point[2]),\n",
    "  color = \"red\",\n",
    "  size = 2.5,\n",
    "  pch = 18\n",
    ")\n",
    "```\n",
    "\n",
    "Figure \\@ref(fig:05-upsample-2) shows what happens if we set the background color of \n",
    "each area of the plot to the predictions the $K$-nearest neighbor \n",
    "classifier would make. We can see that the decision is \n",
    "always \"benign,\" corresponding to the blue color.\n",
    "\n",
    "```{r 05-upsample-2, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.cap = \"Imbalanced data with background color indicating the decision of the classifier and the points represent the labeled data.\"}\n",
    "\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "knn_fit <- knn_spec |>\n",
    "  fit(Class ~ ., data = rare_cancer)\n",
    "\n",
    "# create a prediction pt grid\n",
    "per_grid <- seq(min(rare_cancer$Perimeter), \n",
    "                max(rare_cancer$Perimeter), \n",
    "                length.out = 100)\n",
    "con_grid <- seq(min(rare_cancer$Concavity), \n",
    "                max(rare_cancer$Concavity), \n",
    "                length.out = 100)\n",
    "pcgrid <- as_tibble(expand.grid(Perimeter = per_grid, Concavity = con_grid))\n",
    "knnPredGrid <- predict(knn_fit, pcgrid)\n",
    "prediction_table <- bind_cols(knnPredGrid, pcgrid) |> \n",
    "  rename(Class = .pred_class)\n",
    "\n",
    "# create the basic plt\n",
    "rare_plot <-\n",
    "  ggplot() +\n",
    "  geom_point(data = rare_cancer, \n",
    "             mapping = aes(x = Perimeter, \n",
    "                           y = Concavity, \n",
    "                           color = Class), \n",
    "             alpha = 0.75) +\n",
    "  geom_point(data = prediction_table, \n",
    "             mapping = aes(x = Perimeter, \n",
    "                           y = Concavity, \n",
    "                           color = Class), \n",
    "             alpha = 0.02, \n",
    "             size = 5.) +\n",
    "  labs(color = \"Diagnosis\", \n",
    "       x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\") +\n",
    "  scale_color_manual(labels = c(\"Malignant\", \"Benign\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\"))\n",
    "\n",
    "rare_plot\n",
    "```\n",
    "\n",
    "Despite the simplicity of the problem, solving it in a statistically sound manner is actually\n",
    "fairly nuanced, and a careful treatment would require a lot more detail and mathematics than we will cover in this textbook.\n",
    "For the present purposes, it will suffice to rebalance the data by *oversampling* the rare class. \\index{oversampling}\n",
    "In other words, we will replicate rare observations multiple times in our data set to give them more\n",
    "voting power in the $K$-nearest neighbor algorithm. In order to do this, we will add an oversampling\n",
    "step to the earlier `uc_recipe` recipe with the `step_upsample` function from the `themis` R package. \\index{recipe!step\\_upsample}\n",
    "We show below how to do this, and also\n",
    "use the `group_by` and `summarize` functions to see that our classes are now balanced:\n",
    "\n",
    "```{r 05-upsample-cancer, warning = FALSE, message = FALSE}\n",
    "library(themis)\n",
    "\n",
    "ups_recipe <- recipe(Class ~ ., data = rare_cancer) |>\n",
    "  step_upsample(Class, over_ratio = 1, skip = FALSE) |>\n",
    "  prep()\n",
    "\n",
    "ups_recipe\n",
    "```\n",
    "\n",
    "```{r 05-upsample-cancer-2}\n",
    "upsampled_cancer <- bake(ups_recipe, rare_cancer)\n",
    "\n",
    "upsampled_cancer |>\n",
    "  group_by(Class) |>\n",
    "  summarize(n = n())\n",
    "```\n",
    "\n",
    "Now suppose we train our $K$-nearest neighbor classifier with $K=7$ on this *balanced* data. \n",
    "Figure \\@ref(fig:05-upsample-plot) shows what happens now when we set the background color \n",
    "of each area of our scatter plot to the decision the $K$-nearest neighbor \n",
    "classifier would make. We can see that the decision is more reasonable; when the points are close\n",
    "to those labeled malignant, the classifier predicts a malignant tumor, and vice versa when they are \n",
    "closer to the benign tumor observations.\n",
    "\n",
    "```{r 05-upsample-plot, echo = FALSE, fig.height = 3.5, fig.width = 4.5, fig.pos = \"H\", out.extra=\"\", fig.cap = \"Upsampled data with background color indicating the decision of the classifier.\"}\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "knn_fit <- knn_spec |>\n",
    "  fit(Class ~ ., data = upsampled_cancer)\n",
    "\n",
    "# create a prediction pt grid\n",
    "knnPredGrid <- predict(knn_fit, pcgrid)\n",
    "prediction_table <- bind_cols(knnPredGrid, pcgrid) |> \n",
    "  rename(Class = .pred_class)\n",
    "\n",
    "# create the basic plt\n",
    "upsampled_plot <-\n",
    "  ggplot() +\n",
    "  geom_point(data = prediction_table, \n",
    "             mapping = aes(x = Perimeter, \n",
    "                           y = Concavity, \n",
    "                           color = Class), \n",
    "             alpha = 0.02, size = 5.) +\n",
    "  geom_point(data = rare_cancer, \n",
    "             mapping = aes(x = Perimeter, \n",
    "                           y = Concavity, \n",
    "                           color = Class), \n",
    "             alpha = 0.75) +\n",
    "  labs(color = \"Diagnosis\", \n",
    "       x = \"Perimeter (standardized)\", \n",
    "       y = \"Concavity (standardized)\") +\n",
    "  scale_color_manual(labels = c(\"Malignant\", \"Benign\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\"))\n",
    "\n",
    "upsampled_plot\n",
    "```\n",
    "\n",
    "## Putting it together in a `workflow` {#puttingittogetherworkflow}\n",
    "\n",
    "The `tidymodels` package collection also provides the `workflow`, a way to chain\\index{tidymodels!workflow}\\index{workflow|see{tidymodels}} together multiple data analysis steps without a lot of otherwise necessary code for intermediate steps.\n",
    "To illustrate the whole pipeline, let's start from scratch with the `unscaled_wdbc.csv` data.\n",
    "First we will load the data, create a model, and specify a recipe for how the data should be preprocessed:\n",
    "\n",
    "```{r 05-workflow}\n",
    "# load the unscaled cancer data \n",
    "# and make sure the target Class variable is a factor\n",
    "unscaled_cancer <- read_csv(\"data/unscaled_wdbc.csv\") |>\n",
    "  mutate(Class = as_factor(Class))\n",
    "\n",
    "# create the KNN model\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# create the centering / scaling recipe\n",
    "uc_recipe <- recipe(Class ~ Area + Smoothness, data = unscaled_cancer) |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors())\n",
    "```\n",
    "\n",
    "Note that each of these steps is exactly the same as earlier, except for one major difference:\n",
    "we did not use the `select` function to extract the relevant variables from the data frame,\n",
    "and instead simply specified the relevant variables to use via the \n",
    "formula `Class ~ Area + Smoothness` (instead of `Class ~ .`) in the recipe.\n",
    "You will also notice that we did not call `prep()` on the recipe; this is unnecessary when it is\n",
    "placed in a workflow.\n",
    "\n",
    "We will now place these steps in a `workflow` using the `add_recipe` and `add_model` functions, \\index{tidymodels!add\\_recipe}\\index{tidymodels!add\\_model}\n",
    "and finally we will use the `fit` function to run the whole workflow on the `unscaled_cancer` data.\n",
    "Note another difference from earlier here: we do not include a formula in the `fit` function. This \\index{tidymodels!fit}\n",
    "is again because we included the formula in the recipe, so there is no need to respecify it:\n",
    "\n",
    "```{r 05-workflow-add, results = 'hide', echo = TRUE}\n",
    "knn_fit <- workflow() |>\n",
    "  add_recipe(uc_recipe) |>\n",
    "  add_model(knn_spec) |>\n",
    "  fit(data = unscaled_cancer)\n",
    "\n",
    "knn_fit\n",
    "```\n",
    "\n",
    "```{r echo = FALSE}\n",
    "print_tidymodels(knn_fit)\n",
    "```\n",
    "\n",
    "As before, the fit object lists the function that trains the model as well as the \"best\" settings\n",
    "for the number of neighbors and weight function (for now, these are just the values we chose\n",
    " manually when we created `knn_spec` above). But now the fit object also includes information about\n",
    "the overall workflow, including the centering and scaling preprocessing steps.\n",
    "In other words, when we use the `predict` function with the `knn_fit` object to make a prediction for a new\n",
    "observation, it will first apply the same recipe steps to the new observation. \n",
    "As an example, we will predict the class label of two new observations:\n",
    "one with `Area = 500` and `Smoothness = 0.075`, and one with `Area = 1500` and `Smoothness = 0.1`.\n",
    "\n",
    "```{r 05-workflow-predict}\n",
    "new_observation <- tibble(Area = c(500, 1500), Smoothness = c(0.075, 0.1))\n",
    "prediction <- predict(knn_fit, new_observation)\n",
    "\n",
    "prediction\n",
    "```\n",
    "\n",
    "The classifier predicts that the first observation is benign (\"B\"), while the second is\n",
    "malignant (\"M\"). Figure \\@ref(fig:05-workflow-plot-show) visualizes the predictions that this \n",
    "trained $K$-nearest neighbor model will make on a large range of new observations.\n",
    "Although you have seen colored prediction map visualizations like this a few times now,\n",
    "we have not included the code to generate them, as it is a little bit complicated.\n",
    "For the interested reader who wants a learning challenge, we now include it below. \n",
    "The basic idea is to create a grid of synthetic new observations using the `expand.grid` function, \n",
    "predict the label of each, and visualize the predictions with a colored scatter having a very high transparency \n",
    "(low `alpha` value) and large point radius. See if you can figure out what each line is doing!\n",
    "\n",
    "> **Note:** Understanding this code is not required for the remainder of the\n",
    "> textbook. It is included for those readers who would like to use similar\n",
    "> visualizations in their own data analyses. \n",
    "\n",
    "```{r 05-workflow-plot-show, fig.height = 3.5, fig.width = 4.6, fig.cap = \"Scatter plot of smoothness versus area where background color indicates the decision of the classifier.\"}\n",
    "# create the grid of area/smoothness vals, and arrange in a data frame\n",
    "are_grid <- seq(min(unscaled_cancer$Area), \n",
    "                max(unscaled_cancer$Area), \n",
    "                length.out = 100)\n",
    "smo_grid <- seq(min(unscaled_cancer$Smoothness), \n",
    "                max(unscaled_cancer$Smoothness), \n",
    "                length.out = 100)\n",
    "asgrid <- as_tibble(expand.grid(Area = are_grid, \n",
    "                                Smoothness = smo_grid))\n",
    "\n",
    "# use the fit workflow to make predictions at the grid points\n",
    "knnPredGrid <- predict(knn_fit, asgrid)\n",
    "\n",
    "# bind the predictions as a new column with the grid points\n",
    "prediction_table <- bind_cols(knnPredGrid, asgrid) |> \n",
    "  rename(Class = .pred_class)\n",
    "\n",
    "# plot:\n",
    "# 1. the colored scatter of the original data\n",
    "# 2. the faded colored scatter for the grid points\n",
    "wkflw_plot <-\n",
    "  ggplot() +\n",
    "  geom_point(data = unscaled_cancer, \n",
    "             mapping = aes(x = Area, \n",
    "                           y = Smoothness, \n",
    "                           color = Class), \n",
    "             alpha = 0.75) +\n",
    "  geom_point(data = prediction_table, \n",
    "             mapping = aes(x = Area, \n",
    "                           y = Smoothness, \n",
    "                           color = Class), \n",
    "             alpha = 0.02, \n",
    "             size = 5) +\n",
    "  labs(color = \"Diagnosis\", \n",
    "       x = \"Area (standardized)\", \n",
    "       y = \"Smoothness (standardized)\") +\n",
    "  scale_color_manual(labels = c(\"Malignant\", \"Benign\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\")) +\n",
    "  theme(text = element_text(size = 12))\n",
    "\n",
    "wkflw_plot\n",
    "```\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Practice exercises for the material covered in this chapter \n",
    "can be found in the accompanying \n",
    "[worksheets repository](https://github.com/UBC-DSCI/data-science-a-first-intro-worksheets#readme)\n",
    "in the \"Classification I: training and predicting\" row.\n",
    "You can launch an interactive version of the worksheet in your browser by clicking the \"launch binder\" button.\n",
    "You can also preview a non-interactive version of the worksheet by clicking \"view worksheet.\"\n",
    "If you instead decide to download the worksheet and run it on your own machine,\n",
    "make sure to follow the instructions for computer setup\n",
    "found in Chapter \\@ref(move-to-your-own-machine). This will ensure that the automated feedback\n",
    "and guidance that the worksheets provide will function as intended."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,md:myst,ipynb",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "source_map": [
   14,
   80,
   189,
   359,
   385,
   527,
   706
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}